{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset path\n",
    "# # ROOT_DIR = \"/home/miglab/Desktop/Duke_Liver/Converted_Liver_DaTA\"\n",
    "# ROOT_DIR = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/EPS_Liver/Converted_Liver_DaTA\"\n",
    "# IMG_HEIGHT, IMG_WIDTH = 256, 256\n",
    "\n",
    "# # Lists to store images and masks\n",
    "# image_paths, mask_paths = [], []\n",
    "\n",
    "# # Recursively traverse dataset folders\n",
    "# for main_folder in os.listdir(ROOT_DIR):\n",
    "#     main_folder_path = os.path.join(ROOT_DIR, main_folder)\n",
    "    \n",
    "#     if os.path.isdir(main_folder_path):\n",
    "#         for subfolder in os.listdir(main_folder_path):\n",
    "#             subfolder_path = os.path.join(main_folder_path, subfolder)\n",
    "            \n",
    "#             img_dir = os.path.join(subfolder_path, \"images\")\n",
    "#             mask_dir = os.path.join(subfolder_path, \"masks\")\n",
    "\n",
    "#             if os.path.isdir(img_dir) and os.path.isdir(mask_dir):\n",
    "#                 img_files = sorted(os.listdir(img_dir))\n",
    "#                 mask_files = sorted(os.listdir(mask_dir))\n",
    "#                 # print(img_files)\n",
    "#                 # raise Exception\n",
    "#                 for img_file, mask_file in zip(img_files, mask_files):\n",
    "#                     image_paths.append(os.path.join(img_dir, img_file))\n",
    "#                     mask_paths.append(os.path.join(mask_dir, mask_file))\n",
    "#                     # print(image_paths)\n",
    "# print(image_paths[:75])\n",
    "# # print(mask_paths[:50])\n",
    "# print(f\"Total Images: {len(image_paths)}, Total Masks: {len(mask_paths)}\")\n",
    "\n",
    "# # Split data into train and validation sets\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(image_paths, mask_paths, test_size=0.2, random_state=42)\n",
    "# print(f\"Train: {len(X_train)}, Validation: {len(X_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "\n",
    "# # Define the output folder where new structure will be created\n",
    "# output_folder = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/EPS_Medical/Liver_dataset\"\n",
    "\n",
    "# # Define target folders\n",
    "# leftImg8bit_train = os.path.join(output_folder, \"leftImg8bit/train\")\n",
    "# leftImg8bit_val = os.path.join(output_folder, \"leftImg8bit/val\")\n",
    "# gtFine_train = os.path.join(output_folder, \"gtFine/train\")\n",
    "# gtFine_val = os.path.join(output_folder, \"gtFine/val\")\n",
    "\n",
    "# # Create directories if they don't exist\n",
    "# for folder in [leftImg8bit_train, leftImg8bit_val, gtFine_train, gtFine_val]:\n",
    "#     os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# # Function to generate a unique filename\n",
    "# def generate_new_filename(original_path, replace_text):\n",
    "#     parts = original_path.split('/')\n",
    "#     folder1, folder2, file_name = parts[-4], parts[-3], parts[-1]\n",
    "#     new_name = f\"{folder1}_{folder2}_{replace_text}_{file_name}\"\n",
    "#     return new_name\n",
    "\n",
    "# # Function to copy, resize, convert to RGB, and rename files\n",
    "# def copy_resize_and_rename(file_list, destination_folder, replace_text):\n",
    "#     for file_path in file_list:\n",
    "#         if not os.path.exists(file_path):  # Skip missing files\n",
    "#             continue\n",
    "\n",
    "#         # Load the image in grayscale mode\n",
    "#         image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "#         # Convert grayscale to RGB by duplicating channels\n",
    "#         rgb_image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "#         # Resize using nearest-neighbor interpolation (preserves pixel values)\n",
    "#         resized_image = cv2.resize(rgb_image, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#         # Generate new filename\n",
    "#         new_name = generate_new_filename(file_path, replace_text)\n",
    "#         new_path = os.path.join(destination_folder, new_name)\n",
    "\n",
    "#         # Save the resized RGB image\n",
    "#         cv2.imwrite(new_path, resized_image)\n",
    "\n",
    "# # Copy, resize, and convert images to RGB\n",
    "# copy_resize_and_rename(X_train, leftImg8bit_train, \"images\")\n",
    "# copy_resize_and_rename(X_val, leftImg8bit_val, \"images\")\n",
    "\n",
    "# # Copy and resize masks without conversion to RGB (assuming they remain grayscale)\n",
    "# copy_resize_and_rename(Y_train, gtFine_train, \"masks\")\n",
    "# copy_resize_and_rename(Y_val, gtFine_val, \"masks\")\n",
    "\n",
    "# print(\"Files successfully resized, converted to RGB, and reorganized!\")\n",
    "\n",
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the output folder where new structure will be created\n",
    "# output_folder = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/EPS_Medical/Liver_dataset\"\n",
    "\n",
    "# # Define target folders\n",
    "# leftImg8bit_train = os.path.join(output_folder, \"leftImg8bit/train\")\n",
    "# leftImg8bit_val = os.path.join(output_folder, \"leftImg8bit/val\")\n",
    "# gtFine_train = os.path.join(output_folder, \"gtFine/train\")\n",
    "# gtFine_val = os.path.join(output_folder, \"gtFine/val\")\n",
    "\n",
    "# # Create directories if they don't exist\n",
    "# for folder in [leftImg8bit_train, leftImg8bit_val, gtFine_train, gtFine_val]:\n",
    "#     os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# # Function to generate a unique filename\n",
    "# def generate_new_filename(original_path, replace_text):\n",
    "#     parts = original_path.split('/')\n",
    "#     folder1, folder2, file_name = parts[-4], parts[-3], parts[-1]\n",
    "#     new_name = f\"{folder1}_{folder2}_{replace_text}_{file_name}\"\n",
    "#     return new_name\n",
    "\n",
    "# # Function to copy, resize, and optionally modify image/mask values\n",
    "# def copy_resize_and_rename(file_list, destination_folder, replace_text):\n",
    "#     for file_path in file_list:\n",
    "#         if not os.path.exists(file_path):  # Skip missing files\n",
    "#             continue\n",
    "\n",
    "#         # Load the image in grayscale mode\n",
    "#         image = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "#         if replace_text == \"images\":\n",
    "#             processed_image = image  # Keep grayscale as-is\n",
    "#         else:\n",
    "#             # Convert 0 -> 1 (background), 255 -> 2001 (liver)\n",
    "#             processed_image = np.where(image == 255, 2001, 1).astype(np.uint16)\n",
    "\n",
    "#         # Resize using nearest-neighbor to preserve label values\n",
    "#         resized_image = cv2.resize(processed_image, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "#         # Generate new filename\n",
    "#         new_name = generate_new_filename(file_path, replace_text)\n",
    "#         new_path = os.path.join(destination_folder, new_name)\n",
    "\n",
    "#         # Save\n",
    "#         if replace_text == \"images\":\n",
    "#             cv2.imwrite(new_path, resized_image)  # Grayscale, no channel change\n",
    "#         else:\n",
    "#             cv2.imwrite(new_path, resized_image)  # Save masks as uint16\n",
    "\n",
    "# # Copy, resize, and keep grayscale images\n",
    "# copy_resize_and_rename(X_train, leftImg8bit_train, \"images\")\n",
    "# copy_resize_and_rename(X_val, leftImg8bit_val, \"images\")\n",
    "\n",
    "# # Copy and resize masks with value modifications\n",
    "# copy_resize_and_rename(Y_train, gtFine_train, \"masks\")\n",
    "# copy_resize_and_rename(Y_val, gtFine_val, \"masks\")\n",
    "\n",
    "# print(\"âœ… Files successfully resized, kept in grayscale, and masks remapped and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the image\n",
    "# image_path = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/EPS_Medical/Liver_dataset/leftImg8bit/val/0001_17_images_0020.png\"\n",
    "# image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)  # Load in original format\n",
    "\n",
    "# # Print the shape\n",
    "# print(\"Image shape:\", image.shape)\n",
    "\n",
    "# # Check if the image is grayscale or RGB\n",
    "# if len(image.shape) == 2:  # Grayscale image\n",
    "#     print(\"The image is grayscale.\")\n",
    "#     unique_values = np.unique(image)\n",
    "#     print(\"Unique pixel values:\", unique_values)\n",
    "    \n",
    "# elif len(image.shape) == 3 and image.shape[2] == 3:  # RGB image\n",
    "#     print(\"The image is RGB.\")\n",
    "#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "#     unique_colors = np.unique(image_rgb.reshape(-1, 3), axis=0)\n",
    "#     print(\"Unique colors in the image:\", unique_colors)\n",
    "# else:\n",
    "#     print(\"Unknown image format.\")\n",
    "\n",
    "# image_to_save = image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/EPS_Medical/Liver_dataset/output/cityscapes_panoptic_val/0001_17_masks_0020_panoptic.png\"\n",
    "# image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED) \n",
    "# gt_to_save = image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# # Load the image\n",
    "# image_path = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/EPS_Liver/Liver_dataset_EPS/gtFine/val/0001_34_panoptic_0055.png\"\n",
    "# image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED) \n",
    "\n",
    "# print(image.shape)\n",
    "# # raise Exception\n",
    "# mask = image == 255\n",
    "# color = [255, 0, 0]\n",
    "# pan_format = np.zeros(\n",
    "#                 (256, 256, 3), dtype=np.uint8\n",
    "#             )\n",
    "# pan_format[mask] = color\n",
    "\n",
    "# area = np.sum(mask) # segment area computation\n",
    "\n",
    "# # bbox computation for a segment\n",
    "# hor = np.sum(mask, axis=0)\n",
    "# hor_idx = np.nonzero(hor)[0]\n",
    "# x = hor_idx[0]\n",
    "# width = hor_idx[-1] - x + 1\n",
    "# vert = np.sum(mask, axis=1)\n",
    "# vert_idx = np.nonzero(vert)[0]\n",
    "# y = vert_idx[0]\n",
    "# height = vert_idx[-1] - y + 1\n",
    "# bbox = [int(x), int(y), int(width), int(height)]\n",
    "\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# pan_format_cv = np.array(pan_format)\n",
    "# # cv2.rectangle(pan_format_cv, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "# cv2.rectangle(pan_format_cv, (bbox[0], bbox[1]), (bbox[0] + bbox[2] - 1, bbox[1] + bbox[3] - 1), (0, 255, 0), 2)\n",
    "\n",
    "# print(bbox)\n",
    "# # Convert back to PIL image (Optional, if you need to use PIL again)\n",
    "# pan_format_with_bbox = Image.fromarray(pan_format_cv)\n",
    "\n",
    "# # Display using Matplotlib (since cv2.imshow() may not work in Ubuntu)\n",
    "# plt.imshow(pan_format_with_bbox)\n",
    "# plt.axis(\"off\")  # Hide axes\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    class MODEL_CUSTOM:\n",
    "        class BACKBONE:\n",
    "            NAME = 'convnextv2_base'  # Change to another model like 'convnextv2_base'\n",
    "            LOAD_PRETRAIN = True  # Don't load pre-trained weights\n",
    "            INPUT_HEIGHT = 256\n",
    "            INPUT_WIDTH = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as F\n",
    "from detectron2.structures import Instances, BitMasks, Boxes\n",
    "from torchvision.transforms import functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2id(color):\n",
    "    \"\"\" Pass the image from RGB to the instance id value\n",
    "    See COCO format doc https://cocodataset.org/#format-data\n",
    "    \"\"\"\n",
    "    if isinstance(color, np.ndarray) and len(color.shape) == 3:\n",
    "        if color.dtype == np.uint8:\n",
    "            color = color.astype(np.int32)\n",
    "        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n",
    "    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n",
    "\n",
    "\n",
    "def coco_to_pascal_bbox(bbox):\n",
    "    return np.stack((bbox[:,0], bbox[:,1],\n",
    "            bbox[:,0]+bbox[:,2], bbox[:,1]+bbox[:,3]), axis=1)\n",
    "\n",
    "\n",
    "def collate_fn(inputs):\n",
    "    return {\n",
    "        'image': torch.stack([TF.to_tensor(i['image']) for i in inputs]),\n",
    "        'semantic': torch.as_tensor([i['semantic'] for i in inputs]),\n",
    "        'instance': [i['instance'] for i in inputs],\n",
    "        'image_id': [i['image_id'] for i in inputs]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint\n",
    ")\n",
    "from detectron2.config import get_cfg, CfgNode\n",
    "from detectron2.utils.events import _CURRENT_STORAGE_STACK, EventStorage\n",
    "import sys\n",
    "\n",
    "\n",
    "# from EfficientPS.datasets.panoptic_dataset import PanopticDataset, collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_custom_param(cfg):\n",
    "    \"\"\"\n",
    "    In order to add custom config parameter in the .yaml those parameter must\n",
    "    be initialised\n",
    "    \"\"\"\n",
    "    # Model\n",
    "    cfg.MODEL_CUSTOM = CfgNode()\n",
    "    cfg.MODEL_CUSTOM.BACKBONE = CfgNode()\n",
    "    cfg.MODEL_CUSTOM.BACKBONE.EFFICIENTNET_ID = 2\n",
    "    cfg.MODEL_CUSTOM.BACKBONE.LOAD_PRETRAIN = True\n",
    "    # DATASET\n",
    "    cfg.NUM_CLASS = 2\n",
    "    cfg.DATASET_PATH = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset\"\n",
    "    cfg.TRAIN_JSON = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/output/cityscapes_panoptic_train.json\"\n",
    "    cfg.VALID_JSON = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/output/cityscapes_panoptic_val.json\"\n",
    "    cfg.PRED_DIR = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/preds\"\n",
    "    cfg.PRED_JSON = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/preds/cityscapes_panoptic_preds.json\"\n",
    "    # Transfom\n",
    "    cfg.TRANSFORM = CfgNode()\n",
    "    cfg.TRANSFORM.NORMALIZE = CfgNode()\n",
    "    cfg.TRANSFORM.NORMALIZE.MEAN = (0.5,)\n",
    "    cfg.TRANSFORM.NORMALIZE.STD  = (0.5,)\n",
    "    cfg.TRANSFORM.RESIZE = CfgNode()\n",
    "    cfg.TRANSFORM.RESIZE.HEIGHT = 256\n",
    "    cfg.TRANSFORM.RESIZE.WIDTH = 256\n",
    "    cfg.TRANSFORM.RANDOMCROP = CfgNode()\n",
    "    cfg.TRANSFORM.RANDOMCROP.HEIGHT = 256\n",
    "    cfg.TRANSFORM.RANDOMCROP.WIDTH = 256\n",
    "    cfg.TRANSFORM.HFLIP = CfgNode()\n",
    "    cfg.TRANSFORM.HFLIP.PROB = 0.5\n",
    "    # Solver\n",
    "    cfg.SOLVER.NAME = \"SGD\"\n",
    "    cfg.SOLVER.ACCUMULATE_GRAD = 1\n",
    "    cfg.SOLVER.MAX_EPOCHS = 40\n",
    "    # Runner\n",
    "    cfg.BATCH_SIZE = 8\n",
    "    cfg.CHECKPOINT_PATH = \"\"\n",
    "    cfg.PRECISION = 32\n",
    "    # Callbacks\n",
    "    cfg.CALLBACKS = CfgNode()\n",
    "    cfg.CALLBACKS.CHECKPOINT_DIR = None\n",
    "    # Inference\n",
    "    cfg.INFERENCE = CfgNode()\n",
    "    cfg.INFERENCE.AREA_TRESH = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # class PanopticcDataset(Dataset):\n",
    "# # #     \"\"\"A dataset for Panotic task\"\"\"\n",
    "\n",
    "# # #     def __init__(self, path_json, root_dir, split, transform=None):\n",
    "# # #         \"\"\"\n",
    "# # #         Args:\n",
    "# # #             csv_file (string): Path to the csv file with annotations.\n",
    "# # #             root_dir (string): Directory with all the images.\n",
    "# # #             transform (callable, optional): Optional transform to be applied\n",
    "# # #                 on a sample.\n",
    "# # #         \"\"\"\n",
    "# # #         self.root_dir = root_dir\n",
    "# # #         self.split = split\n",
    "# # #         self.transform = transform\n",
    "# # #         # Load json file containing information about the dataset\n",
    "# # #         path_file = os.path.join(self.root_dir, path_json)\n",
    "# # #         data = json.load(open(path_file))\n",
    "# # #         annotations = data['annotations']\n",
    "# # #         images = data['images']\n",
    "# # #         self.categories = data['categories']\n",
    "# # #         # print(self.categories)\n",
    "# # #         # raise Exception\n",
    "# # #         # TOiDO Possible problem with VOID label and train_id\n",
    "# # #         # Add mapper to training id and class id\n",
    "# # #         self.semantic_class_mapper = {cat['id']:{\n",
    "# # #                                         'train_id':i,\n",
    "# # #                                         'isthing': cat['isthing']\n",
    "# # #                                     }\n",
    "# # #                                     for i, cat in enumerate(self.categories)}\n",
    "# # #         # print(self.semantic_class_mapper)\n",
    "# # #         self.instance_class_mapper = {cat['id']:{\n",
    "# # #                                         'train_id':i\n",
    "# # #                                     }\n",
    "# # #                                     for i, cat in enumerate(self.categories[1:])}\n",
    "# # #         # print(self.instance_class_mapper)\n",
    "\n",
    "# # #         self.semantic_class_mapper.update({0:{\n",
    "# # #                         'train_id':len(self.categories) +1,\n",
    "# # #                         'isthing':0 }})\n",
    "# # #         # print(self.semantic_class_mapper)\n",
    "# # #         self.semantic_train_id_to_eval_id = [1, 2, 0]\n",
    "# # #         self.instance_train_id_to_eval_id = [2]\n",
    "\n",
    "# # #         # Generate a dictionary with all needed information with idx as key\n",
    "# # #         self.meta_data = {}\n",
    "# # #         for i in range(len(images)):\n",
    "# # #             self.meta_data.update({i:{}})\n",
    "# # #             #TODiO Error Message\n",
    "# # #             assert annotations[i]['image_id'] == images[i]['id']\n",
    "# # #             self.meta_data[i].update({'labelfile_name': annotations[i]['file_name']})\n",
    "# # #             self.meta_data[i].update(annotations[i])\n",
    "# # #             self.meta_data[i].update(images[i])\n",
    "# # #         # print(\"Hello world\")\n",
    "\n",
    "# # #     def __len__(self):\n",
    "# # #         return len(self.meta_data)\n",
    "\n",
    "# # #     def __getitem__(self, idx):\n",
    "# # #         # Retrieve meta data of image\n",
    "# # #         img_data = self.meta_data[idx]\n",
    "# # #         # print(img_data['file_name'])\n",
    "# # #         # Load image\n",
    "# # #         path_img = os.path.join(self.root_dir,\n",
    "# # #                                 'leftImg8bit',\n",
    "# # #                                 self.split,\n",
    "# # #                                 img_data['file_name'].replace('gtFine_', ''))\n",
    "# # #         image = np.asarray(Image.open(path_img))\n",
    "\n",
    "        \n",
    "# # #         # Get label info\n",
    "# # #         path_label = os.path.join(self.root_dir,\n",
    "# # #                                   'output',\n",
    "# # #                                   'cityscapes_panoptic_'+self.split,\n",
    "# # #                                   img_data['labelfile_name'])\n",
    "        \n",
    "# # #         panoptic = np.asarray(Image.open(path_label))\n",
    "    \n",
    "\n",
    "# # #         panoptic = rgb2id(panoptic)\n",
    "# # #         # print(np.unique(panoptic))\n",
    "# # #         # raise Exception\n",
    "# # #         # Get bbox info\n",
    "# # #         rpn_bbox = []\n",
    "# # #         class_bbox = []\n",
    "# # #         for seg in img_data['segments_info']:\n",
    "# # #             seg_category = self.semantic_class_mapper[seg['category_id']]\n",
    "# # #             if seg_category['isthing']:\n",
    "# # #                 rpn_bbox.append(seg[\"bbox\"])\n",
    "# # #                 class_bbox.append(self.instance_class_mapper[seg['category_id']])\n",
    "# # #         class_bbox = [item['train_id'] for item in class_bbox]  \n",
    "\n",
    "# # #         # if len(rpn_bbox) > 0:\n",
    "# # #         #     print(rpn_bbox)\n",
    "# # #         #     bbox = rpn_bbox[0]\n",
    "            \n",
    "# # #         #     # Load image and make it writable\n",
    "# # #         #     panoptic = np.asarray(Image.open(path_label)).copy()  # Ensure it's writable\n",
    "\n",
    "# # #         #     # Draw bounding box\n",
    "# # #         #     cv2.rectangle(panoptic, \n",
    "# # #         #                 (bbox[0], bbox[1]), \n",
    "# # #         #                 (bbox[0] + bbox[2] - 1, bbox[1] + bbox[3] - 1), \n",
    "# # #         #                 (0, 255, 0), 2)\n",
    "\n",
    "# # #         #     # Display image\n",
    "# # #         #     plt.imshow(panoptic)\n",
    "# # #         #     plt.axis(\"off\")  # Hide axis for better visualization\n",
    "# # #         #     plt.show()\n",
    "\n",
    "# # #         #     raise Exception\n",
    "\n",
    "\n",
    "        \n",
    "# # #         # Apply augmentation with albumentations\n",
    "# # #         if self.transform is not None:\n",
    "# # #             transformed = self.transform(\n",
    "# # #                 image=image,\n",
    "# # #                 mask=panoptic,\n",
    "# # #                 bboxes=rpn_bbox,\n",
    "# # #                 class_labels=class_bbox\n",
    "# # #             )\n",
    "# # #             image = transformed['image']\n",
    "# # #             panoptic = transformed['mask']\n",
    "# # #             rpn_bbox = transformed['bboxes']\n",
    "# # #             class_bbox = transformed['class_labels']\n",
    "        \n",
    "# # #         # Create instance class for detectron (Mask RCNN Head)\n",
    "# # #         instance = Instances(panoptic.shape)\n",
    "\n",
    "# # #         # Create semantic segmentation target with augmented data\n",
    "# # #         semantic = np.zeros_like(panoptic, dtype=np.int64)\n",
    "# # #         rpn_mask = np.zeros_like(panoptic)\n",
    "# # #         instance_mask = []\n",
    "# # #         instance_cls = []\n",
    "\n",
    "# # #         for seg in img_data['segments_info']:\n",
    "# # #             seg_category = self.semantic_class_mapper[seg['category_id']]\n",
    "# # #             semantic[panoptic == seg[\"id\"]] = seg_category['train_id']\n",
    "# # #             # If segmentation is a thing generate a mask for maskrcnn target\n",
    "# # #             # Collect information for RPN targets\n",
    "# # #             if seg_category['isthing']:\n",
    "# # #                 seg_category = self.instance_class_mapper[seg['category_id']]\n",
    "# # #                 mask = np.zeros_like(panoptic)\n",
    "# # #                 mask[panoptic == seg[\"id\"]] = 1 #seg_category['train_id']\n",
    "# # #                 instance_cls.append(seg_category['train_id'])\n",
    "# # #                 instance_mask.append(mask)\n",
    "# # #                 # RPN targets\n",
    "# # #                 rpn_mask[panoptic == seg[\"id\"]] = 1\n",
    "\n",
    "# # #         # Create same size of bbox and mask instance\n",
    "# # #         if len(rpn_bbox) > 0:\n",
    "# # #             rpn_bbox = coco_to_pascal_bbox(np.stack([*rpn_bbox]))\n",
    "\n",
    "# # #             instance.gt_masks = BitMasks(instance_mask)\n",
    "# # #             instance.gt_classes = torch.as_tensor(instance_cls)\n",
    "# # #             instance.gt_boxes = Boxes(rpn_bbox)\n",
    "# # #         else:\n",
    "# # #             instance.gt_masks = BitMasks(torch.Tensor([]).view(0,1,1))\n",
    "# # #             instance.gt_classes = torch.as_tensor([])\n",
    "# # #             instance.gt_boxes = Boxes([])\n",
    "            \n",
    "# # #         return {\n",
    "# # #             'image': np.array(image),\n",
    "# # #             'semantic': semantic,\n",
    "# # #             'instance': instance,\n",
    "# # #             'image_id': img_data['image_id']\n",
    "# # #         }\n",
    "# class PanopticcDataset(Dataset):\n",
    "#     \"\"\"A dataset for Panotic task\"\"\"\n",
    "\n",
    "#     def __init__(self, path_json, root_dir, split, transform=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             csv_file (string): Path to the csv file with annotations.\n",
    "#             root_dir (string): Directory with all the images.\n",
    "#             transform (callable, optional): Optional transform to be applied\n",
    "#                 on a sample.\n",
    "#         \"\"\"\n",
    "#         self.root_dir = root_dir\n",
    "#         self.split = split\n",
    "#         self.transform = transform\n",
    "#         # Load json file containing information about the dataset\n",
    "#         path_file = os.path.join(self.root_dir, path_json)\n",
    "#         data = json.load(open(path_file))\n",
    "#         annotations = data['annotations']\n",
    "#         images = data['images']\n",
    "#         self.categories = data['categories']\n",
    "#         # print(self.categories)\n",
    "#         # raise Exception\n",
    "#         # TOiDO Possible problem with VOID label and train_id\n",
    "#         # Add mapper to training id and class id\n",
    "#         self.semantic_class_mapper = {cat['id']:{\n",
    "#                                         'train_id':i,\n",
    "#                                         'isthing': cat['isthing']\n",
    "#                                     }\n",
    "#                                     for i, cat in enumerate(self.categories)}\n",
    "#         # print(self.semantic_class_mapper)\n",
    "#         self.instance_class_mapper = {cat['id']:{\n",
    "#                                         'train_id':i\n",
    "#                                     }\n",
    "#                                     for i, cat in enumerate(self.categories[1:])}\n",
    "#         # print(self.instance_class_mapper)\n",
    "\n",
    "#         self.semantic_class_mapper.update({0:{\n",
    "#                         'train_id':len(self.categories) +1,\n",
    "#                         'isthing':0 }})\n",
    "#         # print(self.semantic_class_mapper)\n",
    "#         self.semantic_train_id_to_eval_id = [1, 2, 0]\n",
    "#         self.instance_train_id_to_eval_id = [2]\n",
    "\n",
    "#         # Generate a dictionary with all needed information with idx as key\n",
    "#         self.meta_data = {}\n",
    "#         for i in range(len(images)):\n",
    "#             self.meta_data.update({i:{}})\n",
    "#             #TODiO Error Message\n",
    "#             assert annotations[i]['image_id'] == images[i]['id']\n",
    "#             self.meta_data[i].update({'labelfile_name': annotations[i]['file_name']})\n",
    "#             self.meta_data[i].update(annotations[i])\n",
    "#             self.meta_data[i].update(images[i])\n",
    "#         # print(\"Hello world\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.meta_data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Retrieve meta data of image\n",
    "#         img_data = self.meta_data[idx]\n",
    "#         # print(img_data['file_name'])\n",
    "#         # Load image\n",
    "#         path_img = os.path.join(self.root_dir,\n",
    "#                                 'leftImg8bit',\n",
    "#                                 self.split,\n",
    "#                                 img_data['file_name'].replace('gtFine_', ''))\n",
    "#         image = np.asarray(Image.open(path_img))\n",
    "\n",
    "        \n",
    "#         # Get label info\n",
    "#         path_label = os.path.join(self.root_dir,\n",
    "#                                   'output',\n",
    "#                                   'cityscapes_panoptic_'+self.split,\n",
    "#                                   img_data['labelfile_name'])\n",
    "        \n",
    "#         panoptic = np.asarray(Image.open(path_label))\n",
    "    \n",
    "\n",
    "#         panoptic = rgb2id(panoptic)\n",
    "#         # print(np.unique(panoptic))\n",
    "#         # raise Exception\n",
    "#         # Get bbox info\n",
    "#         rpn_bbox = []\n",
    "#         class_bbox = []\n",
    "#         for seg in img_data['segments_info']:\n",
    "#             seg_category = self.semantic_class_mapper[seg['category_id']]\n",
    "#             if seg_category['isthing']:\n",
    "#                 rpn_bbox.append(seg[\"bbox\"])\n",
    "#                 class_bbox.append(self.instance_class_mapper[seg['category_id']])\n",
    "#         class_bbox = [item['train_id'] for item in class_bbox]  \n",
    "\n",
    "#         # if len(rpn_bbox) > 0:_panoptic\n",
    "#         #     print(rpn_bbox)\n",
    "#         #     bbox = rpn_bbox[0]\n",
    "            \n",
    "#         #     # Load image and make it writable\n",
    "#         #     panoptic = np.asarray(Image.open(path_label)).copy()  # Ensure it's writable\n",
    "\n",
    "#         #     # Draw bounding box\n",
    "#         #     cv2.rectangle(panoptic, \n",
    "#         #                 (bbox[0], bbox[1]), \n",
    "#         #                 (bbox[0] + bbox[2] - 1, bbox[1] + bbox[3] - 1), \n",
    "#         #                 (0, 255, 0), 2)\n",
    "\n",
    "#         #     # Display image\n",
    "#         #     plt.imshow(panoptic)\n",
    "#         #     plt.axis(\"off\")  # Hide axis for better visualization\n",
    "#         #     plt.show()\n",
    "\n",
    "#         #     raise Exception\n",
    "\n",
    "\n",
    "        \n",
    "#         # Apply augmentation with albumentations\n",
    "#         if self.transform is not None:\n",
    "#             transformed = self.transform(\n",
    "#                 image=image,\n",
    "#                 mask=panoptic,\n",
    "#                 bboxes=rpn_bbox,\n",
    "#                 class_labels=class_bbox\n",
    "#             )\n",
    "#             image = transformed['image']\n",
    "#             panoptic = transformed['mask']\n",
    "#             rpn_bbox = transformed['bboxes']\n",
    "#             class_bbox = transformed['class_labels']\n",
    "        \n",
    "#         # Create instance class for detectron (Mask RCNN Head)\n",
    "#         instance = Instances(panoptic.shape)\n",
    "\n",
    "#         # Create semantic segmentation target with augmented data\n",
    "#         semantic = np.zeros_like(panoptic, dtype=np.int64)\n",
    "#         rpn_mask = np.zeros_like(panoptic)\n",
    "#         instance_mask = []\n",
    "#         instance_cls = []\n",
    "\n",
    "#         for seg in img_data['segments_info']:\n",
    "#             seg_category = self.semantic_class_mapper[seg['category_id']]\n",
    "#             semantic[panoptic == seg[\"id\"]] = seg_category['train_id']\n",
    "#             # If segmentation is a thing generate a mask for maskrcnn target\n",
    "#             # Collect information for RPN targets\n",
    "#             if seg_category['isthing']:\n",
    "#                 seg_category = self.instance_class_mapper[seg['category_id']]\n",
    "#                 mask = np.zeros_like(panoptic)\n",
    "#                 mask[panoptic == seg[\"id\"]] = 1 #seg_category['train_id']\n",
    "#                 instance_cls.append(seg_category['train_id'])\n",
    "#                 instance_mask.append(mask)\n",
    "#                 # RPN targets\n",
    "#                 rpn_mask[panoptic == seg[\"id\"]] = 1\n",
    "\n",
    "#         # Create same size of bbox and mask instance\n",
    "#         if len(rpn_bbox) > 0:\n",
    "#             rpn_bbox = coco_to_pascal_bbox(np.stack([*rpn_bbox]))\n",
    "\n",
    "#             instance.gt_masks = BitMasks(instance_mask)\n",
    "#             instance.gt_classes = torch.as_tensor(instance_cls)\n",
    "#             instance.gt_boxes = Boxes(rpn_bbox)\n",
    "#         else:\n",
    "#             instance.gt_masks = BitMasks(torch.Tensor([]).view(0,1,1))\n",
    "#             instance.gt_classes = torch.as_tensor([])\n",
    "#             instance.gt_boxes = Boxes([])\n",
    "        \n",
    "#         print(img_data['image_id'])\n",
    "#         return {\n",
    "#             'image': np.array(image),\n",
    "#             'semantic': semantic,\n",
    "#             'instance': instance,\n",
    "#             'image_id': img_data['image_id']\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanopticcDataset(Dataset):\n",
    "    \"\"\"A dataset for Panotic task\"\"\"\n",
    "\n",
    "    def __init__(self, path_json, root_dir, split, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        # Load json file containing information about the dataset\n",
    "        path_file = os.path.join(self.root_dir, path_json)\n",
    "        data = json.load(open(path_file))\n",
    "        annotations = data['annotations']\n",
    "        images = data['images']\n",
    "        self.categories = data['categories']\n",
    "        # print(self.categories)\n",
    "        # raise Exception\n",
    "        # TOiDO Possible problem with VOID label and train_id\n",
    "        # Add mapper to training id and class id\n",
    "        self.semantic_class_mapper = {cat['id']:{\n",
    "                                        'train_id':i,\n",
    "                                        'isthing': cat['isthing']\n",
    "                                    }\n",
    "                                    for i, cat in enumerate(self.categories)}\n",
    "        # print(self.semantic_class_mapper)\n",
    "        self.instance_class_mapper = {cat['id']:{\n",
    "                                        'train_id':i\n",
    "                                    }\n",
    "                                    for i, cat in enumerate(self.categories[1:])}\n",
    "        # print(self.instance_class_mapper)\n",
    "\n",
    "        self.semantic_class_mapper.update({0:{\n",
    "                        'train_id':len(self.categories) +1,\n",
    "                        'isthing':0 }})\n",
    "        # print(self.semantic_class_mapper)\n",
    "        self.semantic_train_id_to_eval_id = [1, 2, 0]\n",
    "        self.instance_train_id_to_eval_id = [2]\n",
    "\n",
    "        # Generate a dictionary with all needed information with idx as key\n",
    "        self.meta_data = {}\n",
    "        for i in range(len(images)):\n",
    "            self.meta_data.update({i:{}})\n",
    "            #TODiO Error Message\n",
    "            assert annotations[i]['image_id'] == images[i]['id']\n",
    "            self.meta_data[i].update({'labelfile_name': annotations[i]['file_name']})\n",
    "            self.meta_data[i].update(annotations[i])\n",
    "            self.meta_data[i].update(images[i])\n",
    "        # print(\"Hello world\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve meta data of image\n",
    "        img_data = self.meta_data[idx]\n",
    "        # print(img_data['file_name'])\n",
    "        # Load image\n",
    "        path_img = os.path.join(self.root_dir,\n",
    "                                'leftImg8bit',\n",
    "                                self.split,\n",
    "                                img_data['file_name'].replace('gtFine_', ''))\n",
    "        image = np.asarray(Image.open(path_img))\n",
    "\n",
    "        \n",
    "        # Get label info\n",
    "        path_label = os.path.join(self.root_dir,\n",
    "                                  'output',\n",
    "                                  'cityscapes_panoptic_'+self.split,\n",
    "                                  img_data['labelfile_name'])\n",
    "        \n",
    "        panoptic = np.asarray(Image.open(path_label))\n",
    "    \n",
    "\n",
    "        panoptic = rgb2id(panoptic)\n",
    "        # print(np.unique(panoptic))\n",
    "        # raise Exception\n",
    "        # Get bbox info\n",
    "        rpn_bbox = []\n",
    "        class_bbox = []\n",
    "        for seg in img_data['segments_info']:\n",
    "            seg_category = self.semantic_class_mapper[seg['category_id']]\n",
    "            if seg_category['isthing']:\n",
    "                rpn_bbox.append(seg[\"bbox\"])\n",
    "                class_bbox.append(self.instance_class_mapper[seg['category_id']])\n",
    "        class_bbox = [item['train_id'] for item in class_bbox]  \n",
    "\n",
    "        # if len(rpn_bbox) > 0:\n",
    "        #     print(rpn_bbox)\n",
    "        #     bbox = rpn_bbox[0]\n",
    "            \n",
    "        #     # Load image and make it writable\n",
    "        #     panoptic = np.asarray(Image.open(path_label)).copy()  # Ensure it's writable\n",
    "\n",
    "        #     # Draw bounding box\n",
    "        #     cv2.rectangle(panoptic, \n",
    "        #                 (bbox[0], bbox[1]), \n",
    "        #                 (bbox[0] + bbox[2] - 1, bbox[1] + bbox[3] - 1), \n",
    "        #                 (0, 255, 0), 2)\n",
    "\n",
    "        #     # Display image\n",
    "        #     plt.imshow(panoptic)\n",
    "        #     plt.axis(\"off\")  # Hide axis for better visualization\n",
    "        #     plt.show()\n",
    "\n",
    "        #     raise Exception\n",
    "\n",
    "\n",
    "        \n",
    "        # Apply augmentation with albumentations\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                mask=panoptic,\n",
    "                bboxes=rpn_bbox,\n",
    "                class_labels=class_bbox\n",
    "            )\n",
    "            image = transformed['image']\n",
    "            panoptic = transformed['mask']\n",
    "            rpn_bbox = transformed['bboxes']\n",
    "            class_bbox = transformed['class_labels']\n",
    "        \n",
    "        # Create instance class for detectron (Mask RCNN Head)\n",
    "        instance = Instances(panoptic.shape)\n",
    "\n",
    "        # Create semantic segmentation target with augmented data\n",
    "        semantic = np.zeros_like(panoptic, dtype=np.int64)\n",
    "        rpn_mask = np.zeros_like(panoptic)\n",
    "        instance_mask = []\n",
    "        instance_cls = []\n",
    "\n",
    "        for seg in img_data['segments_info']:\n",
    "            seg_category = self.semantic_class_mapper[seg['category_id']]\n",
    "            semantic[panoptic == seg[\"id\"]] = seg_category['train_id']\n",
    "            # If segmentation is a thing generate a mask for maskrcnn target\n",
    "            # Collect information for RPN targets\n",
    "            if seg_category['isthing']:\n",
    "                seg_category = self.instance_class_mapper[seg['category_id']]\n",
    "                mask = np.zeros_like(panoptic)\n",
    "                mask[panoptic == seg[\"id\"]] = 1 #seg_category['train_id']\n",
    "                instance_cls.append(seg_category['train_id'])\n",
    "                instance_mask.append(mask)\n",
    "                # RPN targets\n",
    "                rpn_mask[panoptic == seg[\"id\"]] = 1\n",
    "\n",
    "        # Create same size of bbox and mask instance\n",
    "        if len(rpn_bbox) > 0:\n",
    "            rpn_bbox = coco_to_pascal_bbox(np.stack([*rpn_bbox]))\n",
    "\n",
    "            instance.gt_masks = BitMasks(instance_mask)\n",
    "            instance.gt_classes = torch.as_tensor(instance_cls)\n",
    "            instance.gt_boxes = Boxes(rpn_bbox)\n",
    "        else:\n",
    "            instance.gt_masks = BitMasks(torch.Tensor([]).view(0,1,1))\n",
    "            instance.gt_classes = torch.as_tensor([])\n",
    "            instance.gt_boxes = Boxes([])\n",
    "            \n",
    "        return {\n",
    "            'image': np.array(image),\n",
    "            'semantic': semantic,\n",
    "            'instance': instance,\n",
    "            'image_id': img_data['image_id']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.read_csv(\"EPS_MEDICAL/Liver_dataset/metrics_scores_liver_new.csv\").head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changes saved successfully to cityscapes_panoptic_val_modified.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/output/cityscapes_panoptic_val copy.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Modify the 'file_name' in each annotation\n",
    "for each_file in data['annotations']:\n",
    "    each_file['file_name'] = each_file['file_name'][:-13] + each_file['file_name'][-4:]\n",
    "\n",
    "# Save the modified data back to the JSON file\n",
    "with open('/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/output/cityscapes_panoptic_val_modified.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)  # Pretty-printing the JSON data\n",
    "\n",
    "print(\"Changes saved successfully to cityscapes_panoptic_val_modified.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config '/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/EfficientPS/config.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n",
      "MODEL:\n",
      "  ANCHOR_GENERATOR:\n",
      "    SIZES: [[32], [64], [128], [256]]  # One size for each in feature map\n",
      "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]  # Three aspect ratios (same for all in feature maps)\n",
      "  PROPOSAL_GENERATOR:\n",
      "    NAME: \"RPNCustom\"\n",
      "  RPN:\n",
      "    HEAD_NAME: \"DepthwiseSepRPNHead\" # Normal RPN Head \"StandardRPNHead\"\n",
      "    IN_FEATURES: [\"P_4\", \"P_8\", \"P_16\", \"P_32\"]\n",
      "    PRE_NMS_TOPK_TRAIN: 2000  # Per FPN level\n",
      "    PRE_NMS_TOPK_TEST: 2000  # Per FPN level\n",
      "    BBOX_REG_LOSS_TYPE: \"smooth_l1\"\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    SMOOTH_L1_BETA: 0.11111111 # 1.0 / 9.0\n",
      "\n",
      "    # Detectron1 uses 2000 proposals per-batch,\n",
      "    # (See \"modeling/rpn/rpn_outputs.py\" for details of this legacy issue)\n",
      "    # which is approximately 1000 proposals per-image since the default batch size for FPN is 2.\n",
      "    POST_NMS_TOPK_TRAIN: 1000\n",
      "    POST_NMS_TOPK_TEST: 1000\n",
      "    SMOOTH_L1_BETA: 0.1111\n",
      "    IOU_THRESHOLDS: [0.3, 0.7]\n",
      "  ROI_HEADS:\n",
      "    NAME: \"CustomROIHeads\"\n",
      "    # BATCH_SIZE_PER_IMAGE: 256 # number of proposals to sample for training\n",
      "    # POSITIVE_FRACTION: 0.25 # fraction of positive (foreground) proposals to sample for training.\n",
      "    IN_FEATURES: [\"P_4\", \"P_8\", \"P_16\", \"P_32\"]\n",
      "    NUM_CLASSES: 2 # There is 8 instance in the city scape dataset\n",
      "    # PROPOSAL_APPEND_GT:\n",
      "    IOU_THRESHOLDS: [0.5]\n",
      "    # IOU_LABELS:\n",
      "    SCORE_THRESH_TEST: 0.5 # First step of panoptic fusion module\n",
      "    NMS_THRESH_TEST: 0.5 # Second step of panoptic fusion module\n",
      "  ROI_BOX_HEAD:\n",
      "    POOLER_RESOLUTION: 7\n",
      "    POOLER_SAMPLING_RATIO: 2 # (maybe put to 2) The `sampling_ratio` parameter for the ROIAlign op.\n",
      "    POOLER_TYPE: \"ROIAlign\" # \"ROIAlignV2\"\n",
      "    SMOOTH_L1_BETA: 1.0\n",
      "    # SCORE_THRESH_TEST: 0.05\n",
      "    # NMS_THRESH_TEST: 0.5\n",
      "    BBOX_REG_LOSS_TYPE: \"smooth_l1\"\n",
      "    SMOOTH_L1_BETA: 1.0\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "  ROI_MASK_HEAD:\n",
      "    POOLER_RESOLUTION: 28  # 14 undedhi\n",
      "    POOLER_TYPE: \"ROIAlign\"\n",
      "TEST:\n",
      "  DETECTIONS_PER_IMAGE: 100\n",
      "\n",
      "#### CUSTOM PARAMETER  #####\n",
      "\n",
      "# DATA\n",
      "# Path to cityscapes dataset\n",
      "DATASET_PATH: \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset\"\n",
      "TRAIN_JSON: \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/output/cityscapes_panoptic_train.json\"\n",
      "VALID_JSON: \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/output/cityscapes_panoptic_val.json\"\n",
      "PRED_DIR: \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/preds\" # Path of images generated in the dataset folder\n",
      "PRED_JSON: \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/preds/cityscapes_panoptic_preds.json\" # Path in the dataset folde of the prediction json created\n",
      "\n",
      "# TRANSFORM based on albumentation https://albumentations.ai/\n",
      "TRANSFORM:\n",
      "  NORMALIZE:\n",
      "    MEAN: (0.5,)\n",
      "    STD: (0.5,)\n",
      "  RESIZE:\n",
      "    HEIGHT: 256\n",
      "    WIDTH: 256\n",
      "  RANDOMCROP:\n",
      "    HEIGHT: 256\n",
      "    WIDTH: 256\n",
      "  HFLIP:\n",
      "    PROB: 0.5\n",
      "\n",
      "# Solver\n",
      "SOLVER:\n",
      "  NAME: \"Adam\" # Adam or SGD\n",
      "  BASE_LR: 1.3e-3\n",
      "  WEIGHT_DECAY: 0.0001 # Only for SGD\n",
      "  WARMUP_ITERS: 500 # Set to 0 for no warmup\n",
      "  ACCUMULATE_GRAD: 1 # Number of accumulated epochs for accumulated gradient\n",
      "\n",
      "CALLBACKS:\n",
      "  CHECKPOINT_DIR: \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/logs\"\n",
      "\n",
      "# Path to load a model\n",
      "CHECKPOINT_PATH: \"\"\n",
      "BATCH_SIZE: 8\n",
      "PRECISION: 16 # Bit precision for mix precision training\n",
      "NUM_CLASS: 2\n",
      "MODEL_CUSTOM:\n",
      "  BACKBONE:\n",
      "    EFFICIENTNET_ID: 2 # Id of the EfficienNet model\n",
      "    LOAD_PRETRAIN: False # Load pretrained EfficienNet model\n",
      "INFERENCE:\n",
      "  AREA_TRESH: 32 #1024 / 2 because it's made on image of resize size\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "add_custom_param(cfg)\n",
    "cfg.merge_from_file(\"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/EfficientPS/config.yaml\")\n",
    "\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)\n",
    "logger = logging.getLogger(\"pytorch_lightning.core\")\n",
    "if not os.path.exists(cfg.CALLBACKS.CHECKPOINT_DIR):\n",
    "    os.makedirs(cfg.CALLBACKS.CHECKPOINT_DIR)\n",
    "logger.addHandler(logging.FileHandler(\n",
    "    os.path.join(cfg.CALLBACKS.CHECKPOINT_DIR,\"core.log\")))\n",
    "with open(\"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/EfficientPS/config.yaml\") as file:\n",
    "    logger.info(file.read())\n",
    "# Initialise Custom storage to avoid error when using detectron 2\n",
    "_CURRENT_STORAGE_STACK.append(EventStorage())\n",
    "\n",
    "# Create transforms\n",
    "transform_train = A.Compose([\n",
    "    A.Resize(height=cfg.TRANSFORM.RESIZE.HEIGHT,\n",
    "                width=cfg.TRANSFORM.RESIZE.WIDTH),\n",
    "    A.RandomCrop(height=cfg.TRANSFORM.RANDOMCROP.HEIGHT,\n",
    "                    width=cfg.TRANSFORM.RANDOMCROP.WIDTH),\n",
    "    A.HorizontalFlip(p=cfg.TRANSFORM.HFLIP.PROB),\n",
    "    A.Normalize(mean=cfg.TRANSFORM.NORMALIZE.MEAN,\n",
    "                std=cfg.TRANSFORM.NORMALIZE.STD),\n",
    "    # A.RandomScale(scale_limit=[0.5, 2]),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n",
    "\n",
    "transform_valid = A.Compose([\n",
    "    A.Resize(height=256, width=256),\n",
    "    A.Normalize(mean=cfg.TRANSFORM.NORMALIZE.MEAN,\n",
    "                std=cfg.TRANSFORM.NORMALIZE.STD),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n",
    "\n",
    "# Create Dataset\n",
    "train_dataset = PanopticcDataset(cfg.TRAIN_JSON,\n",
    "                                cfg.DATASET_PATH,\n",
    "                                'train',\n",
    "                                transform=transform_train)\n",
    "\n",
    "valid_dataset = PanopticcDataset(cfg.VALID_JSON,\n",
    "                                 \n",
    "                                cfg.DATASET_PATH,\n",
    "                                'val',\n",
    "                                transform=transform_valid)\n",
    "\n",
    "# Create Data Loader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False,\n",
    "    num_workers=4\n",
    ")\n",
    "# cfg = get_cfg()\n",
    "# add_custom_param(cfg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cfg.merge_from_file(\"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/EPS_Medical/EfficientPS/config.yaml\")\n",
    "\n",
    "# logging.getLogger(\"pytorch_lightning\").setLevel(logging.INFO)\n",
    "# logger = logging.getLogger(\"pytorch_lightning.core\")\n",
    "# if not os.path.exists(cfg.CALLBACKS.CHECKPOINT_DIR):\n",
    "#     os.makedirs(cfg.CALLBACKS.CHECKPOINT_DIR)\n",
    "# logger.addHandler(logging.FileHandler(\n",
    "#     os.path.join(cfg.CALLBACKS.CHECKPOINT_DIR,\"core.log\")))\n",
    "# with open(\"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/EPS_Medical/EfficientPS/config.yaml\") as file:\n",
    "#     logger.info(file.read())\n",
    "# # Initialise Custom storage to avoid error when using detectron 2\n",
    "# _CURRENT_STORAGE_STACK.append(EventStorage())\n",
    "\n",
    "# # Create transforms\n",
    "# transform_train = A.Compose([\n",
    "#     A.Resize(height=cfg.TRANSFORM.RESIZE.HEIGHT,\n",
    "#                 width=cfg.TRANSFORM.RESIZE.WIDTH),\n",
    "#     A.RandomCrop(height=cfg.TRANSFORM.RANDOMCROP.HEIGHT,\n",
    "#                     width=cfg.TRANSFORM.RANDOMCROP.WIDTH),\n",
    "#     A.HorizontalFlip(p=cfg.TRANSFORM.HFLIP.PROB),\n",
    "#     A.Normalize(mean=cfg.TRANSFORM.NORMALIZE.MEAN,\n",
    "#                 std=cfg.TRANSFORM.NORMALIZE.STD),\n",
    "#     # A.RandomScale(scale_limit=[0.5, 2]),\n",
    "# ], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n",
    "\n",
    "# transform_valid = A.Compose([\n",
    "#     A.Resize(height=256, width=256),\n",
    "#     A.Normalize(mean=cfg.TRANSFORM.NORMALIZE.MEAN,\n",
    "#                 std=cfg.TRANSFORM.NORMALIZE.STD),\n",
    "# ], bbox_params=A.BboxParams(format='coco', label_fields=['class_labels']))\n",
    "\n",
    "# # Create Dataset\n",
    "# train_dataset = PanopticcDataset(cfg.TRAIN_JSON,\n",
    "#                                 cfg.DATASET_PATH,\n",
    "#                                 'train',\n",
    "#                                 transform=transform_train)\n",
    "\n",
    "# valid_dataset = PanopticcDataset(cfg.VALID_JSON,\n",
    "#                                 cfg.DATASET_PATH,\n",
    "#                                 'val',\n",
    "#                                 transform=transform_valid)\n",
    "\n",
    "# # Create Data Loader\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=cfg.BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=collate_fn,\n",
    "#     pin_memory=False,\n",
    "#     num_workers=4\n",
    "# )\n",
    "\n",
    "# valid_loader = DataLoader(\n",
    "#     valid_dataset,\n",
    "#     batch_size=cfg.BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=collate_fn,\n",
    "#     pin_memory=False,\n",
    "#     num_workers=4\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(cfg.CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "Creating a new model\n",
      "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<bound method LightningModule.print of EffificientPS(\n",
      "  (backbone): ConvNeXtV2Backbone(\n",
      "    (backbone): ConvNeXt(\n",
      "      (stem): Sequential(\n",
      "        (0): Conv2d(1, 192, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (stages): Sequential(\n",
      "        (0): ConvNeXtStage(\n",
      "          (downsample): Identity()\n",
      "          (blocks): Sequential(\n",
      "            (0): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): ConvNeXtStage(\n",
      "          (downsample): Sequential(\n",
      "            (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): ConvNeXtStage(\n",
      "          (downsample): Sequential(\n",
      "            (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (3): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (4): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (5): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (6): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (7): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (8): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (9): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (10): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (11): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (12): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (13): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (14): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (15): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (16): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (17): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (18): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (19): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (20): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (21): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (22): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (23): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (24): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (25): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (26): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ConvNeXtStage(\n",
      "          (downsample): Sequential(\n",
      "            (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (1): Conv2d(768, 1536, kernel_size=(2, 2), stride=(2, 2))\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
      "              (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
      "              (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
      "              (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm_pre): Identity()\n",
      "      (head): Identity()\n",
      "    )\n",
      "    (adjust_convs): ModuleDict(\n",
      "      (reduction_1): Sequential(\n",
      "        (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      )\n",
      "      (reduction_2): Conv2d(192, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (reduction_3): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (reduction_4): Conv2d(768, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (reduction_5): Conv2d(1536, 352, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (reduction_6): Conv2d(1536, 1408, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (fpn): TwoWayFpn(\n",
      "    (conv_b_up_x4): Conv2d(24, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_b_up_x4): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_t_dn_x4): Conv2d(24, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_t_dn_x4): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_b_up_x8): Conv2d(48, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_b_up_x8): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_t_dn_x8): Conv2d(48, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_t_dn_x8): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_b_up_x16): Conv2d(120, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_b_up_x16): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_t_dn_x16): Conv2d(120, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_t_dn_x16): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_b_up_x32): Conv2d(352, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_b_up_x32): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_t_dn_x32): Conv2d(352, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_t_dn_x32): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (depth_wise_conv_x4): DepthwiseSeparableConv(\n",
      "      (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "      (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (iabn_out_x4): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (depth_wise_conv_x8): DepthwiseSeparableConv(\n",
      "      (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "      (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (iabn_out_x8): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (depth_wise_conv_x16): DepthwiseSeparableConv(\n",
      "      (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "      (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (iabn_out_x16): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (depth_wise_conv_x32): DepthwiseSeparableConv(\n",
      "      (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "      (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (iabn_out_x32): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "  )\n",
      "  (semantic_head): SemanticHead(\n",
      "    (dpc_x32): DPC(\n",
      "      (conv_first): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 6), dilation=(1, 6), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_first): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 21), dilation=(6, 21), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_2): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_3): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(18, 15), dilation=(18, 15), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_3): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_4): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 3), dilation=(6, 3), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_4): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_last): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (iabn_last): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (dpc_x16): DPC(\n",
      "      (conv_first): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 6), dilation=(1, 6), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_first): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 21), dilation=(6, 21), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_2): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_3): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(18, 15), dilation=(18, 15), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_3): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_4): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 3), dilation=(6, 3), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_4): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_last): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (iabn_last): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (lsfe_x8): LSFE(\n",
      "      (conv_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (conv_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (abn_1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (abn_2): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (lsfe_x4): LSFE(\n",
      "      (conv_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (conv_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (abn_1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (abn_2): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (mc_16_to_8): MC(\n",
      "      (conv_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (conv_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (abn_1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (abn_2): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (mc_8_to_4): MC(\n",
      "      (conv_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (conv_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (abn_1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (abn_2): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (last_conv): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (softmax): Softmax(dim=1)\n",
      "    (cross_entropy_loss): CrossEntropyLoss()\n",
      "  )\n",
      "  (instance_head): InstanceHead(\n",
      "    (rpn): RPNCustom(\n",
      "      (rpn_head): DepthwiseSepRPNHead(\n",
      "        (conv): DepthwiseSeparableConv(\n",
      "          (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "          (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      )\n",
      "      (anchor_generator): DefaultAnchorGenerator(\n",
      "        (cell_anchors): BufferList()\n",
      "      )\n",
      "    )\n",
      "    (roi_heads): CustomROIHeads(\n",
      "      (box_pooler): ROIPooler(\n",
      "        (level_poolers): ModuleList(\n",
      "          (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=2, aligned=False)\n",
      "          (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=2, aligned=False)\n",
      "          (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=2, aligned=False)\n",
      "          (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=2, aligned=False)\n",
      "        )\n",
      "      )\n",
      "      (box_head): BboxNetwork(\n",
      "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "        (first_fc): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "        (first_iabn): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (second_fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (second_iabn): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      )\n",
      "      (box_predictor): FastRCNNOutputLayers(\n",
      "        (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
      "        (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      "      )\n",
      "      (mask_pooler): ROIPooler(\n",
      "        (level_poolers): ModuleList(\n",
      "          (0): ROIAlign(output_size=(28, 28), spatial_scale=0.25, sampling_ratio=2, aligned=False)\n",
      "          (1): ROIAlign(output_size=(28, 28), spatial_scale=0.125, sampling_ratio=2, aligned=False)\n",
      "          (2): ROIAlign(output_size=(28, 28), spatial_scale=0.0625, sampling_ratio=2, aligned=False)\n",
      "          (3): ROIAlign(output_size=(28, 28), spatial_scale=0.03125, sampling_ratio=2, aligned=False)\n",
      "        )\n",
      "      )\n",
      "      (mask_head): MaskNetwork(\n",
      "        (conv_iabn_layers): ModuleList(\n",
      "          (0): DepthwiseSeparableConv(\n",
      "            (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "            (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "          (2): DepthwiseSeparableConv(\n",
      "            (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "            (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (3): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "          (4): DepthwiseSeparableConv(\n",
      "            (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "            (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (5): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "          (6): DepthwiseSeparableConv(\n",
      "            (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "            (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (7): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        )\n",
      "        (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (last_iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (last_conv): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n",
      "<bound method LightningModule.print of EffificientPS(\n",
      "  (backbone): ConvNeXtV2Backbone(\n",
      "    (backbone): ConvNeXt(\n",
      "      (stem): Sequential(\n",
      "        (0): Conv2d(1, 192, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (1): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "      )\n",
      "      (stages): Sequential(\n",
      "        (0): ConvNeXtStage(\n",
      "          (downsample): Identity()\n",
      "          (blocks): Sequential(\n",
      "            (0): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
      "              (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): ConvNeXtStage(\n",
      "          (downsample): Sequential(\n",
      "            (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
      "            (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
      "              (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): ConvNeXtStage(\n",
      "          (downsample): Sequential(\n",
      "            (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
      "            (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (3): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (4): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (5): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (6): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (7): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (8): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (9): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (10): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (11): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (12): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (13): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (14): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (15): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (16): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (17): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (18): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (19): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (20): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (21): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (22): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (23): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (24): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (25): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (26): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
      "              (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): ConvNeXtStage(\n",
      "          (downsample): Sequential(\n",
      "            (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (1): Conv2d(768, 1536, kernel_size=(2, 2), stride=(2, 2))\n",
      "          )\n",
      "          (blocks): Sequential(\n",
      "            (0): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
      "              (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (1): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
      "              (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "            (2): ConvNeXtBlock(\n",
      "              (conv_dw): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536)\n",
      "              (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "              (mlp): GlobalResponseNormMlp(\n",
      "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (act): GELU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (grn): GlobalResponseNorm()\n",
      "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (shortcut): Identity()\n",
      "              (drop_path): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm_pre): Identity()\n",
      "      (head): Identity()\n",
      "    )\n",
      "    (adjust_convs): ModuleDict(\n",
      "      (reduction_1): Sequential(\n",
      "        (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Upsample(scale_factor=2.0, mode='bilinear')\n",
      "      )\n",
      "      (reduction_2): Conv2d(192, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (reduction_3): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (reduction_4): Conv2d(768, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (reduction_5): Conv2d(1536, 352, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (reduction_6): Conv2d(1536, 1408, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (fpn): TwoWayFpn(\n",
      "    (conv_b_up_x4): Conv2d(24, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_b_up_x4): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_t_dn_x4): Conv2d(24, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_t_dn_x4): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_b_up_x8): Conv2d(48, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_b_up_x8): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_t_dn_x8): Conv2d(48, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_t_dn_x8): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_b_up_x16): Conv2d(120, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_b_up_x16): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_t_dn_x16): Conv2d(120, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_t_dn_x16): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_b_up_x32): Conv2d(352, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_b_up_x32): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (conv_t_dn_x32): Conv2d(352, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (iabn_t_dn_x32): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (depth_wise_conv_x4): DepthwiseSeparableConv(\n",
      "      (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "      (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (iabn_out_x4): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (depth_wise_conv_x8): DepthwiseSeparableConv(\n",
      "      (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "      (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (iabn_out_x8): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (depth_wise_conv_x16): DepthwiseSeparableConv(\n",
      "      (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "      (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (iabn_out_x16): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    (depth_wise_conv_x32): DepthwiseSeparableConv(\n",
      "      (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "      (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (iabn_out_x32): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "  )\n",
      "  (semantic_head): SemanticHead(\n",
      "    (dpc_x32): DPC(\n",
      "      (conv_first): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 6), dilation=(1, 6), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_first): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 21), dilation=(6, 21), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_2): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_3): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(18, 15), dilation=(18, 15), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_3): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_4): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 3), dilation=(6, 3), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_4): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_last): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (iabn_last): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (dpc_x16): DPC(\n",
      "      (conv_first): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 6), dilation=(1, 6), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_first): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 21), dilation=(6, 21), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_2): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_3): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(18, 15), dilation=(18, 15), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_3): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_branch_4): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 3), dilation=(6, 3), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (iabn_branch_4): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (conv_last): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (iabn_last): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (lsfe_x8): LSFE(\n",
      "      (conv_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (conv_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (abn_1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (abn_2): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (lsfe_x4): LSFE(\n",
      "      (conv_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (conv_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (abn_1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (abn_2): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (mc_16_to_8): MC(\n",
      "      (conv_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (conv_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (abn_1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (abn_2): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (mc_8_to_4): MC(\n",
      "      (conv_1): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (conv_2): DepthwiseSeparableConv(\n",
      "        (depthwise_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
      "        (iabn): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (pointwise_conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (abn_1): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      (abn_2): InPlaceABN(128, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "    )\n",
      "    (last_conv): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (softmax): Softmax(dim=1)\n",
      "    (cross_entropy_loss): CrossEntropyLoss()\n",
      "  )\n",
      "  (instance_head): InstanceHead(\n",
      "    (rpn): RPNCustom(\n",
      "      (rpn_head): DepthwiseSepRPNHead(\n",
      "        (conv): DepthwiseSeparableConv(\n",
      "          (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "          (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      )\n",
      "      (anchor_generator): DefaultAnchorGenerator(\n",
      "        (cell_anchors): BufferList()\n",
      "      )\n",
      "    )\n",
      "    (roi_heads): CustomROIHeads(\n",
      "      (box_pooler): ROIPooler(\n",
      "        (level_poolers): ModuleList(\n",
      "          (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=2, aligned=False)\n",
      "          (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=2, aligned=False)\n",
      "          (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=2, aligned=False)\n",
      "          (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=2, aligned=False)\n",
      "        )\n",
      "      )\n",
      "      (box_head): BboxNetwork(\n",
      "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "        (first_fc): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "        (first_iabn): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (second_fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (second_iabn): InPlaceABN(1024, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "      )\n",
      "      (box_predictor): FastRCNNOutputLayers(\n",
      "        (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
      "        (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      "      )\n",
      "      (mask_pooler): ROIPooler(\n",
      "        (level_poolers): ModuleList(\n",
      "          (0): ROIAlign(output_size=(28, 28), spatial_scale=0.25, sampling_ratio=2, aligned=False)\n",
      "          (1): ROIAlign(output_size=(28, 28), spatial_scale=0.125, sampling_ratio=2, aligned=False)\n",
      "          (2): ROIAlign(output_size=(28, 28), spatial_scale=0.0625, sampling_ratio=2, aligned=False)\n",
      "          (3): ROIAlign(output_size=(28, 28), spatial_scale=0.03125, sampling_ratio=2, aligned=False)\n",
      "        )\n",
      "      )\n",
      "      (mask_head): MaskNetwork(\n",
      "        (conv_iabn_layers): ModuleList(\n",
      "          (0): DepthwiseSeparableConv(\n",
      "            (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "            (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "          (2): DepthwiseSeparableConv(\n",
      "            (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "            (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (3): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "          (4): DepthwiseSeparableConv(\n",
      "            (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "            (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (5): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "          (6): DepthwiseSeparableConv(\n",
      "            (depthwise_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "            (iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "            (pointwise_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (7): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        )\n",
      "        (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (last_iabn): InPlaceABN(256, eps=1e-05, momentum=0.1, affine=True, activation=leaky_relu[0.01])\n",
      "        (last_conv): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Create model or load a checkpoint\n",
    "\n",
    "labels_dir = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/EfficientPS\"\n",
    "\n",
    "# Add the directory to sys.path\n",
    "if labels_dir not in sys.path:\n",
    "    sys.path.append(labels_dir)\n",
    "\n",
    "from efficientps.panoptic_segmentation_module import *\n",
    "from efficientps import EffificientPS\n",
    "from efficientps.panoptic_metrics import generate_pred_panoptic\n",
    "\n",
    "\n",
    "if os.path.exists(cfg.CHECKPOINT_PATH):\n",
    "    print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')\n",
    "    print(\"Loading model from {}\".format(cfg.CHECKPOINT_PATH))\n",
    "    print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')\n",
    "    efficientps = EffificientPS.load_from_checkpoint(cfg=cfg,\n",
    "        checkpoint_path=cfg.CHECKPOINT_PATH)\n",
    "else:\n",
    "    print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')\n",
    "    print(\"Creating a new model\")\n",
    "    print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')\n",
    "    efficientps = EffificientPS(cfg)\n",
    "    cfg.CHECKPOINT_PATH = None\n",
    "\n",
    "logger.info(efficientps.print)\n",
    "# # Callbacks / Hooks\n",
    "early_stopping = EarlyStopping('PQ', patience=5, mode='max')\n",
    "checkpoint = ModelCheckpoint(monitor='PQ',\n",
    "                                mode='max',\n",
    "                                dirpath=cfg.CALLBACKS.CHECKPOINT_DIR,\n",
    "                                save_last=True,\n",
    "                                verbose=True,)\n",
    "\n",
    "# Create a pytorch lighting trainer\n",
    "# trainer = pl.Trainer(\n",
    "#     # weights_summary='full',\n",
    "#     gpus=1,\n",
    "#     num_sanity_val_steps=0,\n",
    "#     # fast_dev_run=True,tion_module import *\n",
    "from efficientps import EffificientPS\n",
    "from efficientps.panoptic_metrics import generate_pred_panoptic\n",
    "\n",
    "\n",
    "# if os.path.exists(cfg.CHECKPOINT_PATH):\n",
    "#     print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')\n",
    "#     print(\"Loading model from {}\".format(cfg.CHECKPOINT_PATH))\n",
    "#     print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')\n",
    "#     efficientps = EffificientPS.load_from_checkpoint(cfg=cfg,\n",
    "#         checkpoint_path=cfg.CHECKPOINT_PATH)\n",
    "# else:\n",
    "#     print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')\n",
    "#     print(\"Creating a new model\")\n",
    "#     print('\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"')\n",
    "#     efficientps = EffificientPS(cfg)\n",
    "#     cfg.CHECKPOINT_PATH = None\n",
    "\n",
    "efficientps = EffificientPS(cfg)\n",
    "\n",
    "logger.info(efficientps.print)\n",
    "# # Callbacks / Hooks\n",
    "early_stopping = EarlyStopping('PQ', patience=5, mode='max')\n",
    "checkpoint = ModelCheckpoint(monitor='PQ',\n",
    "                                mode='max',\n",
    "                                dirpath=cfg.CALLBACKS.CHECKPOINT_DIR,\n",
    "                                save_last=True,\n",
    "                                verbose=True,)\n",
    "\n",
    "# Create a pytorch lighting trainer\n",
    "# trainer = pl.Trainer(\n",
    "#     # weights_summary='full',\n",
    "#     gpus=1,\n",
    "#     num_sanity_val_steps=0,\n",
    "#     # fast_dev_run=True,\n",
    "#     callbacks=[early_stopping, checkpoint],\n",
    "#     precision=cfg.PRECISION,\n",
    "#     resume_from_checkpoint=cfg.CHECKPOINT_PATH,\n",
    "#     gradient_clip_val=15,\n",
    "#     accumulate_grad_batches=cfg.SOLVER.ACCUMULATE_GRAD\n",
    "# )\n",
    "# logger.addHandler(logging.StreamHandler())\n",
    "#     callbacks=[early_stopping, checkpoint],\n",
    "#     precision=cfg.PRECISION,\n",
    "#     resume_from_checkpoint=cfg.CHECKPOINT_PATH,\n",
    "#     gradient_clip_val=15,\n",
    "#     accumulate_grad_batches=cfg.SOLVER.ACCUMULATE_GRAD\n",
    "# )\n",
    "# logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 199,234,224\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in efficientps.backbone.parameters() if p.requires_grad)\n",
    "print(f\"Total Trainable Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new model\n",
      "Total Trainable Parameters: 215,527,375\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "# from contextlib import suppress\n",
    "\n",
    "# with suppress(UserWarning):\n",
    "#     import torch\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from panopticapi.evaluation import pq_compute\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    return 1 - ((2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth))\n",
    "\n",
    "def compute_dice_loss_batch(panoptic_result, batch, gt_folder):\n",
    "    \"\"\"\n",
    "    Compute Dice loss for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        panoptic_result: Tensor of shape (B, H, W), contains predicted instance IDs.\n",
    "        batch: Dictionary containing 'image_id' (list of filenames without extension).\n",
    "        gt_folder: Path to the folder containing ground truth masks.\n",
    "\n",
    "    Returns:\n",
    "        Average Dice loss for the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = panoptic_result.shape[0]\n",
    "    dice_losses = []\n",
    "    device = panoptic_result.device  # Ensure tensors are on the same device\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Convert panoptic prediction to binary mask\n",
    "        pred_mask = np.where(panoptic_result[i].cpu().numpy() == 1, 0, 255).astype(np.uint8)  # Background = 0, Liver = 255\n",
    "\n",
    "        # Read ground truth mask\n",
    "        image_id = batch['image_id'][i]  # Example: '001.png'\n",
    "        gt_path = os.path.join(gt_folder, f\"{image_id}.png\")\n",
    "        gt_mask = cv2.imread(gt_path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        if gt_mask is None:\n",
    "            print(f\"Warning: Missing GT mask for {image_id}\")\n",
    "            continue\n",
    "\n",
    "        # Convert GT mask to binary (same mapping logic)\n",
    "        gt_mask = np.where(gt_mask == 1, 0, 255).astype(np.uint8)\n",
    "        \n",
    "\n",
    "        # Convert to tensors and move to the same device as predictions\n",
    "        gt_tensor = torch.tensor(gt_mask / 255.0, dtype=torch.float32, device=device)  # Normalize\n",
    "        pred_tensor = torch.tensor(pred_mask / 255.0, dtype=torch.float32, device=device)  # Normalize\n",
    "\n",
    "        # Compute Dice loss\n",
    "        loss = dice_loss(gt_tensor, pred_tensor)\n",
    "        dice_losses.append(loss.item())\n",
    "\n",
    "    # Return average Dice loss for the batch\n",
    "    return np.mean(dice_losses) if dice_losses else 0.0\n",
    "\n",
    "\n",
    "\n",
    "# Check if a checkpoint exists\n",
    "if cfg.CHECKPOINT_PATH and os.path.exists(cfg.CHECKPOINT_PATH):\n",
    "    print(f'Loading model from {cfg.CHECKPOINT_PATH}')\n",
    "    efficientps = EffificientPS.load_from_checkpoint(cfg=cfg, checkpoint_path=cfg.CHECKPOINT_PATH)\n",
    "else:\n",
    "    print(\"Creating a new model\")\n",
    "    efficientps = EffificientPS(cfg)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "efficientps.to(device)\n",
    "\n",
    "# Count trainable parameters\n",
    "total_params = sum(p.numel() for p in efficientps.parameters() if p.requires_grad)\n",
    "print(f\"Total Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "# Set optimizer and scheduler\n",
    "optimizer_config = efficientps.configure_optimizers()\n",
    "optimizer = optimizer_config['optimizer']\n",
    "scheduler = optimizer_config['lr_scheduler']\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = cfg.SOLVER.MAX_EPOCHS\n",
    "log_interval = 200  # Print loss every 200 steps\n",
    "\n",
    "# Early Stopping Variables\n",
    "best_val_loss = 0.0\n",
    "patience = 8  # Number of epochs to wait before stopping\n",
    "epochs_without_improvement = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x73d3d7f75ae0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore PyTorch user warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Optional: For reproducibility or device-related warnings\n",
    "torch.autograd.set_detect_anomaly(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "# from inplace_abn import InPlaceABN\n",
    "\n",
    "# # Output channel from layers given by the `extract_endpoints` function of\n",
    "# # efficient net, use to initialize the fpn\n",
    "# output_feature_size = {\n",
    "#     0: [16, 24, 40, 112, 1280],\n",
    "#     1: [16, 24, 40, 112, 1280],\n",
    "#     2: [16, 24, 48, 120, 352, 1408],\n",
    "#     3: [24, 32, 48, 136, 1536],\n",
    "#     4: [24, 32, 56, 160, 1792],\n",
    "#     5: [24, 40, 64, 176, 2048],\n",
    "#     6: [32, 40, 72, 200, 2304],\n",
    "#     7: [32, 48, 80, 224, 2560],\n",
    "#     8: [32, 56, 88, 248, 2816]\n",
    "# }\n",
    "\n",
    "# def generate_backbone_EfficientPS(cfg):\n",
    "#     \"\"\"\n",
    "#     Create an EfficientNet model base on this repository:\n",
    "#     https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "\n",
    "#     Modify the existing Efficientnet base on the EfficientPS paper,\n",
    "#     ie:\n",
    "#     - replace BN and swish with InplaceBN and LeakyRelu\n",
    "#     - remove se (squeeze and excite) blocks\n",
    "#     Args:\n",
    "#     - cdg (Config) : config object\n",
    "#     Return:\n",
    "#     - backbone (nn.Module) : Modify version of the EfficentNet\n",
    "#     \"\"\"\n",
    "\n",
    "#     # if cfg.MODEL_CUSTOM.BACKBONE.LOAD_PRETRAIN:\n",
    "#     #     backbone = EfficientNet.from_pretrained(\n",
    "#     #         'efficientnet-b{}'.format(cfg.MODEL_CUSTOM.BACKBONE.EFFICIENTNET_ID))\n",
    "#     # else:\n",
    "#     #     backbone = EfficientNet.from_name(\n",
    "#     #         'efficientnet-b{}'.format(cfg.MODEL_CUSTOM.BACKBONE.EFFICIENTNET_ID))\n",
    "\n",
    "#     # backbone._bn0 = InPlaceABN(num_features=backbone._bn0.num_features, eps=0.001)\n",
    "#     # backbone._bn1 = InPlaceABN(num_features=backbone._bn1.num_features, eps=0.001)\n",
    "#     # backbone._swish = nn.Identity()\n",
    "#     # for i, block in enumerate(backbone._blocks):\n",
    "#     #     # Remove SE block\n",
    "#     #     block.has_se = False\n",
    "#     #     # Additional step to have the correct number of parameter on compute\n",
    "#     #     block._se_reduce =  nn.Identity()\n",
    "#     #     block._se_expand = nn.Identity()\n",
    "#     #     # Replace BN with Inplace BN (default activation is leaky relu)\n",
    "#     #     if '_bn0' in [name for name, layer in block.named_children()]:\n",
    "#     #         block._bn0 = InPlaceABN(num_features=block._bn0.num_features, eps=0.001)\n",
    "#     #     block._bn1 = InPlaceABN(num_features=block._bn1.num_features, eps=0.001)\n",
    "#     #     block._bn2 = InPlaceABN(num_features=block._bn2.num_features, eps=0.001)\n",
    "\n",
    "#     #     # Remove swish activation since Inplace BN contains the activation layer\n",
    "#     #     block._swish = nn.Identity()\n",
    "\n",
    "#     # return backbone\n",
    "\n",
    "\n",
    "#     if cfg.MODEL_CUSTOM.BACKBONE.LOAD_PRETRAIN:\n",
    "#         backbone = EfficientNet.from_pretrained(\n",
    "#             'efficientnet-b{}'.format(cfg.MODEL_CUSTOM.BACKBONE.EFFICIENTNET_ID))\n",
    "#     else:\n",
    "#         backbone = EfficientNet.from_name(\n",
    "#             'efficientnet-b{}'.format(cfg.MODEL_CUSTOM.BACKBONE.EFFICIENTNET_ID))\n",
    "\n",
    "#     # === Modify input layer for grayscale ===\n",
    "#     conv_stem = backbone._conv_stem\n",
    "#     # backbone._conv_stem = nn.Conv2d(\n",
    "#     #     in_channels=1,\n",
    "#     #     out_channels=conv_stem.out_channels,\n",
    "#     #     kernel_size=conv_stem.kernel_size,\n",
    "#     #     stride=conv_stem.stride,\n",
    "#     #     padding=conv_stem.padding,\n",
    "#     #     bias=conv_stem.bias is not None\n",
    "#     # )\n",
    "#     kernel_size = conv_stem.kernel_size[0]\n",
    "#     stride = conv_stem.stride[0]\n",
    "\n",
    "#     # Manually compute SAME padding\n",
    "#     # SAME padding = floor((stride - 1) + kernel_size - 1) // 2\n",
    "#     padding = (kernel_size - 1) // 2\n",
    "\n",
    "#     backbone._conv_stem = nn.Conv2d(\n",
    "#         in_channels=1,\n",
    "#         out_channels=conv_stem.out_channels,\n",
    "#         kernel_size=kernel_size,\n",
    "#         stride=stride,\n",
    "#         padding=padding,\n",
    "#         bias=conv_stem.bias is not None\n",
    "#     )\n",
    "#     if not cfg.MODEL_CUSTOM.BACKBONE.LOAD_PRETRAIN:\n",
    "#         nn.init.kaiming_normal_(backbone._conv_stem.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "#     # === Replace BN, Swish and SE as before ===\n",
    "#     backbone._bn0 = InPlaceABN(num_features=backbone._bn0.num_features, eps=0.001)\n",
    "#     backbone._bn1 = InPlaceABN(num_features=backbone._bn1.num_features, eps=0.001)\n",
    "#     backbone._swish = nn.Identity()\n",
    "#     for i, block in enumerate(backbone._blocks):\n",
    "#         block.has_se = False\n",
    "#         block._se_reduce = nn.Identity()\n",
    "#         block._se_expand = nn.Identity()\n",
    "#         if '_bn0' in [name for name, layer in block.named_children()]:\n",
    "#             block._bn0 = InPlaceABN(num_features=block._bn0.num_features, eps=0.001)\n",
    "#         block._bn1 = InPlaceABN(num_features=block._bn1.num_features, eps=0.001)\n",
    "#         block._bn2 = InPlaceABN(num_features=block._bn2.num_features, eps=0.001)\n",
    "#         block._swish = nn.Identity()\n",
    "\n",
    "#     return backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "# from inplace_abn import InPlaceABN\n",
    "\n",
    "# # Mock configuration object\n",
    "# class Config:\n",
    "#     class MODEL_CUSTOM:\n",
    "#         class BACKBONE:\n",
    "#             LOAD_PRETRAIN = True  # Use pre-trained weights\n",
    "#             EFFICIENTNET_ID = 2  # Use EfficientNet-B2\n",
    "\n",
    "# cfg = Config()\n",
    "\n",
    "# # Generate the modified EfficientNet backbone\n",
    "# backbone = generate_backbone_EfficientPS(cfg)\n",
    "\n",
    "# # Prepare a dummy input (grayscale image)\n",
    "# batch_size = 1\n",
    "# height, width = 224, 224  # Example input size\n",
    "# input_tensor = torch.randn(batch_size, 1, height, width)  # Grayscale image (1 channel)\n",
    "\n",
    "# # Perform a forward pass\n",
    "# with torch.no_grad():  # Disable gradient computation for inference\n",
    "#     output = backbone.extract_endpoints(input_tensor)\n",
    "\n",
    "# # Inspect the output\n",
    "# for key, value in output.items():\n",
    "#     print(f\"Feature map {key}: Shape = {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output.keys())\n",
    "# print(feature_maps.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import timm\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# # Load and modify the pretrained ConvNeXt V2 Large model for 1-channel input\n",
    "# def load_convnext_v2_large_1channel():\n",
    "#     # Load the pretrained ConvNeXt V2 Large model\n",
    "#     model = timm.create_model('convnextv2_large', pretrained=True)\n",
    "    \n",
    "#     # Modify the first convolutional layer to accept 1 input channel\n",
    "#     original_first_layer = model.stem[0]\n",
    "#     new_first_layer = torch.nn.Conv2d(\n",
    "#         in_channels=1,                  # Change input channels to 1\n",
    "#         out_channels=original_first_layer.out_channels,\n",
    "#         kernel_size=original_first_layer.kernel_size,\n",
    "#         stride=original_first_layer.stride,\n",
    "#         padding=original_first_layer.padding,\n",
    "#         bias=original_first_layer.bias is not None\n",
    "#     )\n",
    "#     # Copy weights from the original layer (use the mean of RGB weights for grayscale)\n",
    "#     with torch.no_grad():\n",
    "#         new_first_layer.weight.data = original_first_layer.weight.data.mean(dim=1, keepdim=True)\n",
    "#     model.stem[0] = new_first_layer\n",
    "    \n",
    "#     # Remove the classification head (outer layers)\n",
    "#     model.head = torch.nn.Identity()\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Hook function to extract feature maps\n",
    "# class FeatureExtractor:\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "#         self.features = OrderedDict()\n",
    "#         self.hooks = []\n",
    "\n",
    "#     def register_hooks(self):\n",
    "#         # List of layers to extract feature maps\n",
    "#         reduction_layers = [\n",
    "#             'stages.0.downsample',      # Reduction 1\n",
    "#             'stages.1.downsample',      # Reduction 2\n",
    "#             'stages.2.downsample',      # Reduction 3\n",
    "#             'stages.3.blocks.0',        # Intermediate layer in Stage 3\n",
    "#             'stages.3.blocks.2'      # Intermediate layer in Stage 3\n",
    "#         ]\n",
    "        \n",
    "#         # Register hooks to capture feature maps\n",
    "#         for i, layer_name in enumerate(reduction_layers):\n",
    "#             layer = dict(self.model.named_modules())[layer_name]\n",
    "#             hook = layer.register_forward_hook(self.get_hook(f'reduction_{i+1}'))\n",
    "#             self.hooks.append(hook)\n",
    "\n",
    "#     def get_hook(self, layer_name):\n",
    "#         def hook(module, input, output):\n",
    "#             self.features[layer_name] = output\n",
    "#         return hook\n",
    "\n",
    "#     def remove_hooks(self):\n",
    "#         for hook in self.hooks:\n",
    "#             hook.remove()\n",
    "\n",
    "# # Function to extract feature maps\n",
    "# def extract_feature_maps(model, input_tensor):\n",
    "#     extractor = FeatureExtractor(model)\n",
    "#     extractor.register_hooks()\n",
    "    \n",
    "#     # Forward pass through the model\n",
    "#     with torch.no_grad():\n",
    "#         _ = model(input_tenimport matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a batch already (like in your previous print statements)\n",
    "# image = samples['image'][0].cpu().squeeze()  # (1, H, W) â†’ (H, W)\n",
    "# plt.imshow(image, cmap='gray')\n",
    "# plt.title(\"Input Image\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "# or)\n",
    "    \n",
    "#     # Get the feature maps\n",
    "#     feature_maps = extractor.features\n",
    "    \n",
    "#     # Remove hooks after extraction\n",
    "#     extractor.remove_hooks()\n",
    "    \n",
    "#     return feature_maps\n",
    "\n",
    "# # Main function to demonstrate the process\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load the modified ConvNeXt V2 Large model for 1-channel input\n",
    "#     model = load_convnext_v2_large_1channel()\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "#     # Create a dummy input tensor (batch_size=1, channels=1, height=224, width=224)\n",
    "#     dummy_input = torch.randn(1, 1, 224, 224)\n",
    "\n",
    "#     # Extract feature maps\n",
    "#     feature_maps = extract_feature_maps(model, dummy_input)\n",
    "\n",
    "#     # Print the keys and shapes of the feature maps\n",
    "#     print(\"Feature Maps Keys:\", feature_maps.keys())\n",
    "#     for key, value in feature_maps.items():\n",
    "#         print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import timm\n",
    "# from collections import OrderedDict\n",
    "# from torch import nn\n",
    "\n",
    "# def generate_backbone_EfficientPS(input_tensor):\n",
    "#     # Load and modify the pretrained ConvNeXt V2 Large model for 1-channel input\n",
    "#     def load_convnext_v2_large_1channel():\n",
    "#         model = timm.create_model('convnextv2_large', pretrained=True)\n",
    "#         original_first_layer = model.stem[0]\n",
    "#         new_first_layer = nn.Conv2d(\n",
    "#             in_channels=1,\n",
    "#             out_channels=original_first_layer.out_channels,\n",
    "#             kernel_size=original_first_layer.kernel_size,\n",
    "#             stride=original_first_layer.stride,\n",
    "#             padding=original_first_layer.padding,\n",
    "#             bias=original_first_layer.bias is not None\n",
    "#         )\n",
    "#         with torch.no_grad():\n",
    "#             new_first_layer.weight.data = original_first_layer.weight.data.mean(dim=1, keepdim=True)\n",
    "#         model.stem[0] = new_first_layer\n",
    "#         model.head = nn.Identity()\n",
    "#         return model\n",
    "\n",
    "#     model = load_convnext_v2_large_1channel()\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Create adjustment layers\n",
    "#     adjust_convs = nn.ModuleList([\n",
    "#         nn.Conv2d(192, 16, kernel_size=1),  # For stem\n",
    "#         nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "#         nn.Conv2d(192, 24, kernel_size=1),  # For stages.0.downsample\n",
    "#         nn.Conv2d(384, 48, kernel_size=1),  # For stages.1.downsample\n",
    "#         nn.Conv2d(768, 120, kernel_size=1),  # For stages.2.downsample\n",
    "#         nn.Conv2d(1536, 352, kernel_size=1),  # For stages.3.blocks.0\n",
    "#         nn.Conv2d(1536, 1408, kernel_size=1)   # For stages.3.blocks.2\n",
    "#     ]).to(input_tensor.device)\n",
    "    \n",
    "#     # Feature extractor class\n",
    "#     class FeatureExtractor:\n",
    "#         def __init__(self, model, adjust_convs):\n",
    "#             self.model = model\n",
    "#             self.adjust_convs = adjust_convs\n",
    "#             self.features = OrderedDict()\n",
    "#             self.hooks = []\n",
    "\n",
    "#         def register_hooks(self):\n",
    "#             reduction_layers = [\n",
    "#                 'stem',\n",
    "#                 'stages.0.downsample',\n",
    "#                 'stages.1.downsample',\n",
    "#                 'stages.2.downsample',\n",
    "#                 'stages.3.blocks.0',\n",
    "#                 'stages.3.blocks.2'\n",
    "#             ]\n",
    "            \n",
    "#             for layer_name in reduction_layers:\n",
    "#                 layer = dict(self.model.named_modules())[layer_name]\n",
    "#                 hook = layer.register_forward_hook(self.get_hook(layer_name))\n",
    "#                 self.hooks.append(hook)\n",
    "\n",
    "#         def get_hook(self, layer_name):\n",
    "#             def hook(module, input, output):\n",
    "#                 with torch.no_grad():\n",
    "#                     if layer_name == 'stem':\n",
    "#                         adjusted = self.adjust_convs[0](output)\n",
    "#                         adjusted = self.adjust_convs[1](adjusted)\n",
    "#                         self.features['reduction_1'] = adjusted\n",
    "#                     elif layer_name == 'stages.0.downsample':\n",
    "#                         adjusted = self.adjust_convs[2](output)\n",
    "#                         self.features['reduction_2'] = adjusted\n",
    "#                     elif layer_name == 'stages.1.downsample':\n",
    "#                         adjusted = self.adjust_convs[3](output)\n",
    "#                         self.features['reduction_3'] = adjusted\n",
    "#                     elif layer_name == 'stages.2.downsample':\n",
    "#                         adjusted = self.adjust_convs[4](output)\n",
    "#                         self.features['reduction_4'] = adjusted\n",
    "#                     elif layer_name == 'stages.3.blocks.0':\n",
    "#                         adjusted = self.adjust_convs[5](output)\n",
    "#                         self.features['reduction_5'] = adjusted\n",
    "#                     elif layer_name == 'stages.3.blocks.2':\n",
    "#                         adjusted = self.adjust_convs[6](output)\n",
    "#                         self.features['reduction_6'] = adjusted\n",
    "#             return hook\n",
    "\n",
    "#         def remove_hooks(self):\n",
    "#             for hook in self.hooks:\n",
    "#                 hook.remove()\n",
    "\n",
    "#     # Extract features\n",
    "#     extractor = FeatureExtractor(model, adjust_convs)\n",
    "#     extractor.register_hooks()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         _ = model(input_tensor)\n",
    "    \n",
    "#     feature_maps = extractor.features\n",
    "#     extractor.remove_hooks()\n",
    "    \n",
    "#     # Order the features\n",
    "#     desired_keys = ['reduction_1', 'reduction_2', 'reduction_3', 'reduction_4', 'reduction_5', 'reduction_6']\n",
    "#     ordered_features = OrderedDict()\n",
    "#     for key in desired_keys:\n",
    "#         ordered_features[key] = feature_maps[key]\n",
    "    \n",
    "#     return ordered_features\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     dummy_input = torch.randn(1, 1, 224, 224)\n",
    "#     feature_maps = generate_backbone_EfficientPS(dummy_input)\n",
    "#     for key, value in feature_maps.items():\n",
    "#         print(f\"{key}: {value.shape}\")\n",
    "#     print(feature_maps.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.INFERENCE.AREA_THRESH =32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/miglab/miniconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs =30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Initialize best validation loss and patience counter\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "metrics_csv_path = os.path.join(cfg.DATASET_PATH, \"metrics_scores_liver_new.csv\")\n",
    "\n",
    "# Ensure the CSV file exists and write the header if it doesn't\n",
    "if not os.path.exists(metrics_csv_path):\n",
    "    with open(metrics_csv_path, \"w\") as f:\n",
    "        f.write(\"Epoch,Train_Loss,Val_Loss,PQ,SQ,RQ,Train_Dice_Coeff,Train_IoU,Train_Accuracy,Train_Dice_Loss,Val_Dice_Coeff,Val_IoU,Val_Accuracy,Val_Dice_Loss\\n\")\n",
    "\n",
    "# Helper function to compute IoU\n",
    "def compute_iou(pred, target):\n",
    "    pred = pred > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "    target = target > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "\n",
    "    intersection = torch.sum(pred * target)\n",
    "    union = torch.sum(pred) + torch.sum(target) - intersection\n",
    "    iou = (intersection / (union + 1e-8)).item()  # Add epsilon to avoid division by zero\n",
    "    return iou\n",
    "\n",
    "# Helper function to compute pixel-wise accuracy\n",
    "def compute_accuracy(pred, target):\n",
    "    pred = pred > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "    target = target > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "\n",
    "    correct = torch.sum(pred == target)\n",
    "    total = torch.numel(target)\n",
    "    accuracy = (correct / total).item()\n",
    "    return accuracy\n",
    "\n",
    "# Clear output for each epoch\n",
    "def clear_output():\n",
    "    sys.stdout.write(\"\\033[F\" * 10)  # Move the cursor up to 10 lines\n",
    "    sys.stdout.write(\"\\033[K\")  # Clear the current line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.55 GiB of which 143.00 MiB is free. Process 2002376 has 20.60 GiB memory in use. Process 2014073 has 298.00 MiB memory in use. Including non-PyTorch memory, this process has 1.41 GiB memory in use. Of the allocated memory 1.11 GiB is allocated by PyTorch, and 59.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m.\u001b[39mgt_boxes \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstance\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\u001b[38;5;241m.\u001b[39mgt_boxes\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m predictions, loss \u001b[38;5;241m=\u001b[39m \u001b[43mefficientps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m panoptic_result \u001b[38;5;241m=\u001b[39m panoptic_segmentation(cfg, predictions, device)\n\u001b[1;32m     29\u001b[0m gt_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPS_MEDICAL/Liver_dataset/gtFine/train\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update this\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/EfficientPS/efficientps/model.py:51\u001b[0m, in \u001b[0;36mEffificientPS.shared_step\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Feature extraction\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_endpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print(features.keys())\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# for i in features.keys():\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#     print(f\"{i} Shape : \", features[i].shape)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m pyramid_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfpn(features)\n",
      "File \u001b[0;32m/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/EfficientPS/efficientps/backbone/modify_efficientnet.py:85\u001b[0m, in \u001b[0;36mConvNeXtV2Backbone.extract_endpoints\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_endpoints\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_outputs\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m---> 85\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/timm/models/convnext.py:507\u001b[0m, in \u001b[0;36mConvNeXt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 507\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/timm/models/convnext.py:499\u001b[0m, in \u001b[0;36mConvNeXt.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    498\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[0;32m--> 499\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_pre(x)\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/timm/models/convnext.py:234\u001b[0m, in \u001b[0;36mConvNeXtStage.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    232\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/timm/models/convnext.py:160\u001b[0m, in \u001b[0;36mConvNeXtBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    158\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    159\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m--> 160\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/timm/layers/mlp.py:264\u001b[0m, in \u001b[0;36mGlobalResponseNormMlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    262\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m    263\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n\u001b[0;32m--> 264\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m    266\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop2(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/timm/layers/grn.py:39\u001b[0m, in \u001b[0;36mGlobalResponseNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m x_g \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_dim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m x_n \u001b[38;5;241m=\u001b[39m x_g \u001b[38;5;241m/\u001b[39m (x_g\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_dim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39maddcmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwb_shape), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwb_shape), \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_n\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.55 GiB of which 143.00 MiB is free. Process 2002376 has 20.60 GiB memory in use. Process 2014073 has 298.00 MiB memory in use. Including non-PyTorch memory, this process has 1.41 GiB memory in use. Of the allocated memory 1.11 GiB is allocated by PyTorch, and 59.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start timing the epoch\n",
    "    \n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "    ### TRAINING LOOP\n",
    "    efficientps.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_dice_loss = 0.0\n",
    "    total_iou_train = 0.0\n",
    "    total_accuracy_train = 0.0\n",
    "\n",
    "    # Wrap the training loop with tqdm for a progress bar\n",
    "    train_loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\", leave=False)\n",
    "    for batch_idx, batch in train_loop:\n",
    "        # Move data to GPU\n",
    "        batch['image'] = batch['image'][:, np.newaxis, 0, :, :].to(device)\n",
    "        batch['semantic'] = batch['semantic'].to(device)\n",
    "\n",
    "        for i in range(len(batch['instance'])):\n",
    "            batch['instance'][i].gt_masks = batch['instance'][i].gt_masks.to(device)\n",
    "            batch['instance'][i].gt_classes = batch['instance'][i].gt_classes.to(device)\n",
    "            batch['instance'][i].gt_boxes = batch['instance'][i].gt_boxes.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions, loss = efficientps.shared_step(batch)\n",
    "\n",
    "        panoptic_result = panoptic_segmentation(cfg, predictions, device)\n",
    "        gt_folder = \"EPS_MEDICAL/Liver_dataset/gtFine/train\"  # Update this\n",
    "        dice_loss_batch = compute_dice_loss_batch(panoptic_result, batch, gt_folder)\n",
    "\n",
    "        # Add Dice Loss explicitly to the loss dictionary\n",
    "        loss.update({'dice_loss': torch.tensor(dice_loss_batch, dtype=torch.float32, device=device)})  # Keeps it on GPU\n",
    "        total_loss = sum(loss.values())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "        total_dice_loss += dice_loss_batch\n",
    "\n",
    "        # Compute IoU and Accuracy for training\n",
    "        iou_train_batch = compute_iou(panoptic_result, batch['semantic'])\n",
    "        accuracy_train_batch = compute_accuracy(panoptic_result, batch['semantic'])\n",
    "\n",
    "        total_iou_train += iou_train_batch\n",
    "        total_accuracy_train += accuracy_train_batch\n",
    "\n",
    "        # Update the progress bar with the current loss\n",
    "        train_loop.set_postfix(train_loss=f\"{total_loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_train_dice_loss = total_dice_loss / len(train_loader)\n",
    "    avg_iou_train = total_iou_train / len(train_loader)\n",
    "    avg_accuracy_train = total_accuracy_train / len(train_loader)\n",
    "\n",
    "    # Apply learning rate scheduling\n",
    "    scheduler.step(avg_train_loss)\n",
    "\n",
    "    ### VALIDATION LOOP\n",
    "    efficientps.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_outputs = []\n",
    "    total_dice_loss = 0.0\n",
    "    total_iou_val = 0.0\n",
    "    total_accuracy_val = 0.0\n",
    "\n",
    "    # Wrap the validation loop with tqdm for a progress bar\n",
    "    val_loop = tqdm(valid_loader, total=len(valid_loader), desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loop:\n",
    "            # Move data to GPU\n",
    "            batch['image'] = batch['image'].to(device)\n",
    "            batch['semantic'] = batch['semantic'].to(device)\n",
    "\n",
    "            for i in range(len(batch['instance'])):\n",
    "                batch['instance'][i].gt_masks = batch['instance'][i].gt_masks.to(device)\n",
    "                batch['instance'][i].gt_classes = batch['instance'][i].gt_classes.to(device)\n",
    "                batch['instance'][i].gt_boxes = batch['instance'][i].gt_boxes.to(device)\n",
    "\n",
    "            # Forward pass (get predictions & loss)\n",
    "            predictions, loss = efficientps.shared_step(batch)\n",
    "            panoptic_result = panoptic_segmentation(cfg, predictions, device)\n",
    "            gt_folder = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/output/cityscapes_panoptic_val\"  # Update this\n",
    "            dice_loss_batch = compute_dice_loss_batch(panoptic_result, batch, gt_folder)\n",
    "\n",
    "            # Compute IoU and Accuracy\n",
    "            iou_val_batch = compute_iou(panoptic_result, batch['semantic'])  # Use semantic mask for IoU\n",
    "            accuracy_val_batch = compute_accuracy(panoptic_result, batch['semantic'])  # Use semantic mask for accuracy\n",
    "\n",
    "            loss.update({'dice_loss': torch.tensor(dice_loss_batch, dtype=torch.float32, device=device)})  # Keeps it on GPU\n",
    "\n",
    "            # Collect validation outputs for PQ computation\n",
    "            val_outputs.append({\n",
    "                'val_loss': sum(loss.values()).item(),\n",
    "                'panoptic': panoptic_result,\n",
    "                'image_id': batch['image_id']\n",
    "            })\n",
    "            total_val_loss += sum(loss.values()).item()\n",
    "            total_dice_loss += dice_loss_batch\n",
    "            total_iou_val += iou_val_batch\n",
    "            total_accuracy_val += accuracy_val_batch\n",
    "\n",
    "            # Update the progress bar with the current loss\n",
    "            val_loop.set_postfix(val_loss=f\"{sum(loss.values()).item():.4f}\")\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(valid_loader)\n",
    "    avg_dice_val_loss = total_dice_loss / len(valid_loader)\n",
    "    avg_iou_val = total_iou_val / len(valid_loader)\n",
    "    avg_accuracy_val = total_accuracy_val / len(valid_loader)\n",
    "\n",
    "    # Compute PQ metric\n",
    "    generate_pred_panoptic(cfg, val_outputs)\n",
    "    pq_res = pq_compute(\n",
    "        gt_json_file=os.path.join(cfg.DATASET_PATH, cfg.VALID_JSON),\n",
    "        pred_json_file=os.path.join(cfg.DATASET_PATH, cfg.PRED_JSON),\n",
    "        gt_folder=os.path.join(cfg.DATASET_PATH, \"output/cityscapes_panoptic_val/\"),\n",
    "        pred_folder=os.path.join(cfg.DATASET_PATH, cfg.PRED_DIR)\n",
    "    )\n",
    "    current_pq = 100 * pq_res['All']['pq']\n",
    "    avg_dice_coeff = 1 - avg_dice_val_loss\n",
    "\n",
    "    # End timing the epoch\n",
    "    epoch_duration = time.time() - start_time\n",
    "\n",
    "    # Print summary for the epoch\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"PQ: {current_pq:.2f}, Dice Coefficient: {avg_dice_coeff:.4f}, \"\n",
    "          f\"IoU: {avg_iou_val:.4f}, Accuracy: {avg_accuracy_val:.4f}, \"\n",
    "          f\"Time Taken: {timedelta(seconds=epoch_duration)}\")\n",
    "\n",
    "    # Log results to CSV\n",
    "    with open(metrics_csv_path, \"a\") as f:\n",
    "        f.write(f\"{epoch+1},{avg_train_loss:.4f},{avg_val_loss:.4f},{current_pq:.2f},\"\n",
    "                f\"{100*pq_res['All']['sq']:.2f},{100*pq_res['All']['rq']:.2f},\"\n",
    "                f\"{avg_iou_train:.4f},{avg_accuracy_train:.4f},{avg_train_dice_loss:.4f},\"\n",
    "                f\"{avg_dice_coeff:.4f},{avg_iou_val:.4f},{avg_accuracy_val:.4f},{avg_dice_val_loss:.4f}\\n\")\n",
    "\n",
    "    print(f\"Metrics logged to {metrics_csv_path}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(efficientps.state_dict(), f\"efficientps_epoch{epoch+1}.pth\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss  # Update best validation loss\n",
    "        epochs_without_improvement = 0  # Reset patience counter\n",
    "        print(\"âœ… Best model saved!\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"ðŸš¨ No improvement for {epochs_without_improvement} epochs.\")\n",
    "\n",
    "    # Stop training if patience is exceeded\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"â¹ï¸ Early stopping triggered after {epoch+1} epochs! Best Validation Loss: {best_val_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "    # Clear output after validation phase\n",
    "    clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import time\n",
    "# from datetime import timedelta\n",
    "# from tqdm import tqdm\n",
    "# import sys\n",
    "\n",
    "# # Initialize best validation loss and patience counter\n",
    "# best_val_loss = float('inf')\n",
    "# epochs_without_improvement = 0\n",
    "# metrics_csv_path = os.path.join(cfg.DATASET_PATH, \"metrics_scores_liver_new.csv\")\n",
    "\n",
    "# # Ensure the CSV file exists and write the header if it doesn't\n",
    "# if not os.path.exists(metrics_csv_path):\n",
    "#     with open(metrics_csv_path, \"w\") as f:\n",
    "#         f.write(\"Epoch,Train_Loss,Val_Loss,PQ,SQ,RQ,Dice_Coeff,IoU,Accuracy\\n\")\n",
    "\n",
    "# # Helper function to compute IoU\n",
    "# def compute_iou(pred, target):\n",
    "#     pred = pred > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "#     target = target > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "\n",
    "#     intersection = torch.sum(pred * target)\n",
    "#     union = torch.sum(pred) + torch.sum(target) - intersection\n",
    "#     iou = (intersection / (union + 1e-8)).item()  # Add epsilon to avoid division by zero\n",
    "#     return iou\n",
    "\n",
    "# # Helper function to compute pixel-wise accuracy\n",
    "# def compute_accuracy(pred, target):\n",
    "#     pred = pred > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "#     target = target > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "\n",
    "#     correct = torch.sum(pred == target)\n",
    "#     total = torch.numel(target)\n",
    "#     accuracy = (correct / total).item()\n",
    "#     return accuracy\n",
    "\n",
    "# # Clear output for each epoch\n",
    "# def clear_output():\n",
    "#     sys.stdout.write(\"\\033[F\" * 10)  # Move the cursor up to 10 lines\n",
    "#     sys.stdout.write(\"\\033[K\")  # Clear the current line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start timing the epoch\n",
    "    \n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "    ### TRAINING LOOP\n",
    "    efficientps.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_dice_loss = 0.0\n",
    "\n",
    "    # Wrap the training loop with tqdm for a progress bar\n",
    "    train_loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\", leave=False)\n",
    "    for batch_idx, batch in train_loop:\n",
    "        # Move data to GPU\n",
    "        batch['image'] = batch['image'][:, np.newaxis, 0, :, :].to(device)\n",
    "        batch['semantic'] = batch['semantic'].to(device)\n",
    "\n",
    "        for i in range(len(batch['instance'])):\n",
    "            batch['instance'][i].gt_masks = batch['instance'][i].gt_masks.to(device)\n",
    "            batch['instance'][i].gt_classes = batch['instance'][i].gt_classes.to(device)\n",
    "            batch['instance'][i].gt_boxes = batch['instance'][i].gt_boxes.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions, loss = efficientps.shared_step(batch)\n",
    "\n",
    "        panoptic_result = panoptic_segmentation(cfg, predictions, device)\n",
    "        gt_folder = \"EPS_MEDICAL/Liver_dataset/gtFine/train\"  # Update this\n",
    "        dice_loss_batch = compute_dice_loss_batch(panoptic_result, batch, gt_folder)\n",
    "\n",
    "        # Add Dice Loss explicitly to the loss dictionary\n",
    "        loss.update({'dice_loss': torch.tensor(dice_loss_batch, dtype=torch.float32, device=device)})  # Keeps it on GPU\n",
    "        total_loss = sum(loss.values())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "        total_dice_loss += dice_loss_batch\n",
    "\n",
    "        # Update the progress bar with the current loss\n",
    "        train_loop.set_postfix(train_loss=f\"{total_loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_train_dice_loss = total_dice_loss / len(train_loader)\n",
    "\n",
    "    # Apply learning rate scheduling\n",
    "    scheduler.step(avg_train_loss)\n",
    "\n",
    "    ### VALIDATION LOOP\n",
    "    efficientps.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_outputs = []\n",
    "    total_dice_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    total_accuracy = 0.0\n",
    "\n",
    "    # Wrap the validation loop with tqdm for a progress bar\n",
    "    val_loop = tqdm(valid_loader, total=len(valid_loader), desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loop:\n",
    "            # Move data to GPU\n",
    "            batch['image'] = batch['image'].to(device)\n",
    "            batch['semantic'] = batch['semantic'].to(device)\n",
    "\n",
    "            for i in range(len(batch['instance'])):\n",
    "                batch['instance'][i].gt_masks = batch['instance'][i].gt_masks.to(device)\n",
    "                batch['instance'][i].gt_classes = batch['instance'][i].gt_classes.to(device)\n",
    "                batch['instance'][i].gt_boxes = batch['instance'][i].gt_boxes.to(device)\n",
    "\n",
    "            # Forward pass (get predictions & loss)\n",
    "            predictions, loss = efficientps.shared_step(batch)\n",
    "            panoptic_result = panoptic_segmentation(cfg, predictions, device)\n",
    "            gt_folder = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/output/cityscapes_panoptic_val\"  # Update this\n",
    "            dice_loss_batch = compute_dice_loss_batch(panoptic_result, batch, gt_folder)\n",
    "\n",
    "            # Compute IoU and Accuracy\n",
    "            iou_batch = compute_iou(panoptic_result, batch['semantic'])  # Use semantic mask for IoU\n",
    "            accuracy_batch = compute_accuracy(panoptic_result, batch['semantic'])  # Use semantic mask for accuracy\n",
    "\n",
    "            loss.update({'dice_loss': torch.tensor(dice_loss_batch, dtype=torch.float32, device=device)})  # Keeps it on GPU\n",
    "\n",
    "            # Collect validation outputs for PQ computation\n",
    "            val_outputs.append({\n",
    "                'val_loss': sum(loss.values()).item(),\n",
    "                'panoptic': panoptic_result,\n",
    "                'image_id': batch['image_id']\n",
    "            })\n",
    "            total_val_loss += sum(loss.values()).item()\n",
    "            total_dice_loss += dice_loss_batch\n",
    "            total_iou += iou_batch\n",
    "            total_accuracy += accuracy_batch\n",
    "\n",
    "            # Update the progress bar with the current loss\n",
    "            val_loop.set_postfix(val_loss=f\"{sum(loss.values()).item():.4f}\")\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(valid_loader)\n",
    "    avg_dice_val_loss = total_dice_loss / len(valid_loader)\n",
    "    avg_iou = total_iou / len(valid_loader)\n",
    "    avg_accuracy = total_accuracy / len(valid_loader)\n",
    "\n",
    "    # Compute PQ metric\n",
    "    generate_pred_panoptic(cfg, val_outputs)\n",
    "    pq_res = pq_compute(\n",
    "        gt_json_file=os.path.join(cfg.DATASET_PATH, cfg.VALID_JSON),\n",
    "        pred_json_file=os.path.join(cfg.DATASET_PATH, cfg.PRED_JSON),\n",
    "        gt_folder=os.path.join(cfg.DATASET_PATH, \"output/cityscapes_panoptic_val/\"),\n",
    "        pred_folder=os.path.join(cfg.DATASET_PATH, cfg.PRED_DIR)\n",
    "    )\n",
    "    current_pq = 100 * pq_res['All']['pq']\n",
    "    avg_dice_coeff = 1 - avg_dice_val_loss\n",
    "\n",
    "    # End timing the epoch\n",
    "    epoch_duration = time.time() - start_time\n",
    "\n",
    "    # Print summary for the epoch\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"PQ: {current_pq:.2f}, Dice Coefficient: {avg_dice_coeff:.4f}, \"\n",
    "          f\"IoU: {avg_iou:.4f}, Accuracy: {avg_accuracy:.4f}, \"\n",
    "          f\"Time Taken: {timedelta(seconds=epoch_duration)}\")\n",
    "\n",
    "    # Log results to CSV\n",
    "    with open(metrics_csv_path, \"a\") as f:\n",
    "        f.write(f\"{epoch+1},{avg_train_loss:.4f},{avg_val_loss:.4f},{current_pq:.2f},\"\n",
    "                f\"{100*pq_res['All']['sq']:.2f},{100*pq_res['All']['rq']:.2f},\"\n",
    "                f\"{avg_dice_coeff:.4f},{avg_iou:.4f},{avg_accuracy:.4f}\\n\")\n",
    "\n",
    "    print(f\"Metrics logged to {metrics_csv_path}\")\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(efficientps.state_dict(), f\"efficientps_epoch{epoch+1}.pth\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss  # Update best validation loss\n",
    "        epochs_without_improvement = 0  # Reset patience counter\n",
    "        print(\"âœ… Best model saved!\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"ðŸš¨ No improvement for {epochs_without_improvement} epochs.\")\n",
    "\n",
    "    # Stop training if patience is exceeded\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"â¹ï¸ Early stopping triggered after {epoch+1} epochs! Best Validation Loss: {best_val_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "    # Clear output after validation phase\n",
    "    clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm  # For progress bars\n",
    "# import time\n",
    "# from datetime import timedelta\n",
    "# import os\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# # Initialize best validation loss and patience counter\n",
    "# best_val_loss = float('inf')\n",
    "# epochs_without_improvement = 0\n",
    "\n",
    "# # Ensure the CSV file exists and write the header if it doesn't\n",
    "# metrics_csv_path = os.path.join(cfg.DATASET_PATH, \"metrics_scores_liver.csv\")\n",
    "# if not os.path.exists(metrics_csv_path):\n",
    "#     with open(metrics_csv_path, \"w\") as f:\n",
    "#         f.write(\"Epoch,Train_Loss,Val_Loss,PQ,SQ,RQ,Dice_Coeff,IoU,Accuracy\\n\")\n",
    "\n",
    "# # Helper function to compute IoU\n",
    "# def compute_iou(pred, target):\n",
    "#     \"\"\"\n",
    "#     Compute Intersection over Union (IoU) between predicted and target masks.\n",
    "#     Args:\n",
    "#         pred: Predicted mask (torch.Tensor or numpy array).\n",
    "#         target: Ground truth mask (torch.Tensor or numpy array).\n",
    "#     Returns:\n",
    "#         iou: Scalar value representing the IoU score.\n",
    "#     \"\"\"\n",
    "#     pred = pred > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "#     target = target > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "\n",
    "#     intersection = torch.sum(pred * target)\n",
    "#     union = torch.sum(pred) + torch.sum(target) - intersection\n",
    "#     iou = (intersection / (union + 1e-8)).item()  # Add epsilon to avoid division by zero\n",
    "#     return iou\n",
    "\n",
    "# # Helper function to compute pixel-wise accuracy\n",
    "# def compute_accuracy(pred, target):\n",
    "#     \"\"\"\n",
    "#     Compute pixel-wise accuracy between predicted and target masks.\n",
    "#     Args:\n",
    "#         pred: Predicted mask (torch.Tensor or numpy array).\n",
    "#         target: Ground truth mask (torch.Tensor or numpy array).\n",
    "#     Returns:\n",
    "#         accuracy: Scalar value representing the pixel-wise accuracy.\n",
    "#     \"\"\"\n",
    "#     pred = pred > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "#     target = target > 0.5  # Convert to binary mask (threshold at 0.5)\n",
    "\n",
    "#     correct = torch.sum(pred == target)\n",
    "#     total = torch.numel(target)\n",
    "#     accuracy = (correct / total).item()\n",
    "#     return accuracy\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     start_time = time.time()  # Start timing the epoch\n",
    "    \n",
    "#     print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "#     ### TRAINING LOOP\n",
    "#     efficientps.train()\n",
    "#     total_train_loss = 0.0\n",
    "#     total_dice_loss = 0.0\n",
    "\n",
    "#     # Wrap the training loop with tqdm for a progress bar\n",
    "#     train_loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\", leave=False)\n",
    "#     for batch_idx, batch in train_loop:\n",
    "#         # Move data to GPU\n",
    "#         # if(batch_idx == 30):\n",
    "#         #     break\n",
    "#         batch['image'] = batch['image'][:, np.newaxis, 0, :, :].to(device)\n",
    "#         batch['semantic'] = batch['semantic'].to(device)\n",
    "\n",
    "#         for i in range(len(batch['instance'])):\n",
    "#             batch['instance'][i].gt_masks = batch['instance'][i].gt_masks.to(device)\n",
    "#             batch['instance'][i].gt_classes = batch['instance'][i].gt_classes.to(device)\n",
    "#             batch['instance'][i].gt_boxes = batch['instance'][i].gt_boxes.to(device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         predictions, loss = efficientps.shared_step(batch)\n",
    "\n",
    "#         panoptic_result = panoptic_segmentation(cfg, predictions, device)\n",
    "#         gt_folder = \"EPS_MEDICAL/Liver_dataset/gtFine/train\"  # Update this\n",
    "#         dice_loss_batch = compute_dice_loss_batch(panoptic_result, batch, gt_folder)\n",
    "\n",
    "#         # Add Dice Loss explicitly to the loss dictionary\n",
    "#         loss.update({'dice_loss': torch.tensor(dice_loss_batch, dtype=torch.float32, device=device)})  # Keeps it on GPU\n",
    "#         total_loss = sum(loss.values())\n",
    "\n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_train_loss += total_loss.item()\n",
    "#         total_dice_loss += dice_loss_batch\n",
    "\n",
    "#         # Update the progress bar with the current loss\n",
    "#         train_loop.set_postfix(train_loss=f\"{total_loss.item():.4f}\")\n",
    "\n",
    "#     avg_train_loss = total_train_loss / len(train_loader)\n",
    "#     avg_train_dice_loss = total_dice_loss / len(train_loader)\n",
    "\n",
    "#     # Apply learning rate scheduling\n",
    "#     scheduler.step(avg_train_loss)\n",
    "\n",
    "#     ### VALIDATION LOOP\n",
    "#     efficientps.eval()\n",
    "#     total_val_loss = 0.0\n",
    "#     val_outputs = []\n",
    "#     total_dice_loss = 0.0\n",
    "#     total_iou = 0.0\n",
    "#     total_accuracy = 0.0\n",
    "\n",
    "#     # Wrap the validation loop with tqdm for a progress bar\n",
    "#     val_loop = tqdm(valid_loader, total=len(valid_loader), desc=\"Validation\", leave=False)\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loop:\n",
    "#             # Move data to GPU\n",
    "#             batch['image'] = batch['image'].to(device)\n",
    "#             batch['semantic'] = batch['semantic'].to(device)\n",
    "\n",
    "#             for i in range(len(batch['instance'])):\n",
    "#                 batch['instance'][i].gt_masks = batch['instance'][i].gt_masks.to(device)\n",
    "#                 batch['instance'][i].gt_classes = batch['instance'][i].gt_classes.to(device)\n",
    "#                 batch['instance'][i].gt_boxes = batch['instance'][i].gt_boxes.to(device)\n",
    "\n",
    "#             # Forward pass (get predictions & loss)\n",
    "#             predictions, loss = efficientps.shared_step(batch)\n",
    "#             panoptic_result = panoptic_segmentation(cfg, predictions, device)\n",
    "#             gt_folder = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/output/cityscapes_panoptic_val\"  # Update this\n",
    "#             dice_loss_batch = compute_dice_loss_batch(panoptic_result, batch, gt_folder)\n",
    "\n",
    "#             # Compute IoU and Accuracy\n",
    "#             iou_batch = compute_iou(panoptic_result, batch['semantic'])  # Use semantic mask for IoU\n",
    "#             accuracy_batch = compute_accuracy(panoptic_result, batch['semantic'])  # Use semantic mask for accuracy\n",
    "\n",
    "#             loss.update({'dice_loss': torch.tensor(dice_loss_batch, dtype=torch.float32, device=device)})  # Keeps it on GPU\n",
    "\n",
    "#             # Collect validation outputs for PQ computation\n",
    "#             val_outputs.append({\n",
    "#                 'val_loss': sum(loss.values()).item(),\n",
    "#                 'panoptic': panoptic_result,\n",
    "#                 'image_id': batch['image_id']\n",
    "#             })\n",
    "#             total_val_loss += sum(loss.values()).item()\n",
    "#             total_dice_loss += dice_loss_batch\n",
    "#             total_iou += iou_batch\n",
    "#             total_accuracy += accuracy_batch\n",
    "\n",
    "#             # Update the progress bar with the current loss\n",
    "#             val_loop.set_postfix(val_loss=f\"{sum(loss.values()).item():.4f}\")\n",
    "\n",
    "#     avg_val_loss = total_val_loss / len(valid_loader)\n",
    "#     avg_dice_val_loss = total_dice_loss / len(valid_loader)\n",
    "#     avg_iou = total_iou / len(valid_loader)\n",
    "#     avg_accuracy = total_accuracy / len(valid_loader)\n",
    "\n",
    "#     # Compute PQ metric\n",
    "#     generate_pred_panoptic(cfg, val_outputs)\n",
    "#     pq_res = pq_compute(\n",
    "#         gt_json_file=os.path.join(cfg.DATASET_PATH, cfg.VALID_JSON),\n",
    "#         pred_json_file=os.path.join(cfg.DATASET_PATH, cfg.PRED_JSON),\n",
    "#         gt_folder=os.path.join(cfg.DATASET_PATH, \"output/cityscapes_panoptic_val/\"),\n",
    "#         pred_folder=os.path.join(cfg.DATASET_PATH, cfg.PRED_DIR)\n",
    "#     )\n",
    "#     current_pq = 100 * pq_res['All']['pq']\n",
    "#     avg_dice_coeff = 1 - avg_dice_val_loss\n",
    "\n",
    "#     # End timing the epoch\n",
    "#     epoch_duration = time.time() - start_time\n",
    "\n",
    "#     # Print summary for the epoch\n",
    "#     print(f\"Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, \"\n",
    "#           f\"PQ: {current_pq:.2f}, Dice Coefficient: {avg_dice_coeff:.4f}, \"\n",
    "#           f\"IoU: {avg_iou:.4f}, Accuracy: {avg_accuracy:.4f}, \"\n",
    "#           f\"Time Taken: {timedelta(seconds=epoch_duration)}\")\n",
    "\n",
    "#     # Log results to CSV\n",
    "#     with open(metrics_csv_path, \"a\") as f:\n",
    "#         f.write(f\"{epoch+1},{avg_train_loss:.4f},{avg_val_loss:.4f},{current_pq:.2f},\"\n",
    "#                 f\"{100*pq_res['All']['sq']:.2f},{100*pq_res['All']['rq']:.2f},\"\n",
    "#                 f\"{avg_dice_coeff:.4f},{avg_iou:.4f},{avg_accuracy:.4f}\\n\")\n",
    "\n",
    "#     print(f\"Metrics logged to {metrics_csv_path}\")\n",
    "\n",
    "#     # Save model checkpoint\n",
    "#     torch.save(efficientps.state_dict(), f\"efficientps_epoch{epoch+1}.pth\")\n",
    "\n",
    "#     # Early stopping logic\n",
    "#     if avg_val_loss < best_val_loss:\n",
    "#         best_val_loss = avg_val_loss  # Update best validation loss\n",
    "#         epochs_without_improvement = 0  # Reset patience counter\n",
    "#         print(\"âœ… Best model saved!\")\n",
    "#     else:\n",
    "#         epochs_without_improvement += 1\n",
    "#         print(f\"ðŸš¨ No improvement for {epochs_without_improvement} epochs.\")\n",
    "\n",
    "#     # Stop training if patience is exceeded\n",
    "#     if epochs_without_improvement >= patience:\n",
    "#         print(f\"â¹ï¸ Early stopping triggered after {epoch+1} epochs! Best Validation Loss: {best_val_loss:.4f}\")\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm  # For progress bars\n",
    "# import time\n",
    "# from datetime import timedelta\n",
    "# import os\n",
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# # Initialize best validation loss and patience counter\n",
    "# best_val_loss = float('inf')\n",
    "# epochs_without_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensure the CSV file exists and write the updated header if it doesn't\n",
    "# metrics_csv_path = os.path.join(cfg.DATASET_PATH, \"metrics_scores_liver_1.csv\")\n",
    "# if not os.path.exists(metrics_csv_path):\n",
    "#     with open(metrics_csv_path, \"w\") as f:\n",
    "#         f.write(\"Epoch,Train_Loss,Val_Loss,PQ,SQ,RQ,\"\n",
    "#                 \"Train_Dice_Coeff,Train_IoU,Train_Accuracy,Train_Dice_Loss,\"\n",
    "#                 \"Val_Dice_Coeff,Val_IoU,Val_Accuracy,Val_Dice_Loss\\n\")\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     start_time = time.time()\n",
    "#     print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "#     ### TRAINING LOOP\n",
    "#     efficientps.train()\n",
    "#     total_train_loss = 0.0\n",
    "#     total_train_dice_loss = 0.0\n",
    "#     total_train_iou = 0.0\n",
    "#     total_train_accuracy = 0.0\n",
    "\n",
    "#     train_loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\", leave=False)\n",
    "#     for batch_idx, batch in train_loop:\n",
    "#         if(batch_idx == 30):\n",
    "#             break\n",
    "        \n",
    "#         batch['image'] = batch['image'][:, np.newaxis, 0, :, :].to(device)\n",
    "#         batch['semantic'] = batch['semantic'].to(device)\n",
    "\n",
    "#         for i in range(len(batch['instance'])):\n",
    "#             batch['instance'][i].gt_masks = batch['instance'][i].gt_masks.to(device)\n",
    "#             batch['instance'][i].gt_classes = batch['instance'][i].gt_classes.to(device)\n",
    "#             batch['instance'][i].gt_boxes = batch['instance'][i].gt_boxes.to(device)\n",
    "\n",
    "#         predictions, loss = efficientps.shared_step(batch)\n",
    "\n",
    "#         panoptic_result = panoptic_segmentation(cfg, predictions, device)\n",
    "#         gt_folder = \"EPS_MEDICAL/Liver_dataset/gtFine/train\"  # Update this\n",
    "#         dice_loss_batch = compute_dice_loss_batch(panoptic_result, batch, gt_folder)\n",
    "\n",
    "#         # Compute IoU and Accuracy for training batch\n",
    "#         iou_batch = compute_iou(panoptic_result, batch['semantic'])\n",
    "#         accuracy_batch = compute_accuracy(panoptic_result, batch['semantic'])\n",
    "\n",
    "#         loss.update({'dice_loss': torch.tensor(dice_loss_batch, dtype=torch.float32, device=device)})\n",
    "#         total_loss = sum(loss.values())\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_train_loss += total_loss.item()\n",
    "#         total_train_dice_loss += dice_loss_batch\n",
    "#         total_train_iou += iou_batch\n",
    "#         total_train_accuracy += accuracy_batch\n",
    "\n",
    "#         train_loop.set_postfix(train_loss=f\"{total_loss.item():.4f}\")\n",
    "\n",
    "#     avg_train_loss = total_train_loss / len(train_loader)\n",
    "#     avg_train_dice_loss = total_train_dice_loss / len(train_loader)\n",
    "#     avg_train_iou = total_train_iou / len(train_loader)\n",
    "#     avg_train_accuracy = total_train_accuracy / len(train_loader)\n",
    "#     avg_train_dice_coeff = 1 - avg_train_dice_loss\n",
    "\n",
    "#     scheduler.step(avg_train_loss)\n",
    "\n",
    "#     ### VALIDATION LOOP\n",
    "#     efficientps.eval()\n",
    "#     total_val_loss = 0.0\n",
    "#     total_val_dice_loss = 0.0\n",
    "#     total_val_iou = 0.0\n",
    "#     total_val_accuracy = 0.0\n",
    "#     val_outputs = []\n",
    "\n",
    "#     val_loop = tqdm(valid_loader, total=len(valid_loader), desc=\"Validation\", leave=False)\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loop:\n",
    "#             batch['image'] = batch['image'].to(device)\n",
    "#             batch['semantic'] = batch['semantic'].to(device)\n",
    "\n",
    "#             for i in range(len(batch['instance'])):\n",
    "#                 batch['instance'][i].gt_masks = batch['instance'][i].gt_masks.to(device)\n",
    "#                 batch['instance'][i].gt_classes = batch['instance'][i].gt_classes.to(device)\n",
    "#                 batch['instance'][i].gt_boxes = batch['instance'][i].gt_boxes.to(device)\n",
    "\n",
    "#             predictions, loss = efficientps.shared_step(batch)\n",
    "#             panoptic_result = panoptic_segmentation(cfg, predictions, device)\n",
    "#             gt_folder = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/EPS_MEDICAL/Liver_dataset/output/cityscapes_panoptic_val\"  # Update this\n",
    "#             dice_loss_batch = compute_dice_loss_batch(panoptic_result, batch, gt_folder)\n",
    "\n",
    "#             iou_batch = compute_iou(panoptic_result, batch['semantic'])\n",
    "#             accuracy_batch = compute_accuracy(panoptic_result, batch['semantic'])\n",
    "\n",
    "#             loss.update({'dice_loss': torch.tensor(dice_loss_batch, dtype=torch.float32, device=device)})\n",
    "\n",
    "#             val_outputs.append({\n",
    "#                 'val_loss': sum(loss.values()).item(),\n",
    "#                 'panoptic': panoptic_result,\n",
    "#                 'image_id': batch['image_id']\n",
    "#             })\n",
    "#             total_val_loss += sum(loss.values()).item()\n",
    "#             total_val_dice_loss += dice_loss_batch\n",
    "#             total_val_iou += iou_batch\n",
    "#             total_val_accuracy += accuracy_batch\n",
    "\n",
    "#             val_loop.set_postfix(val_loss=f\"{sum(loss.values()).item():.4f}\")\n",
    "\n",
    "#     avg_val_loss = total_val_loss / len(valid_loader)\n",
    "#     avg_val_dice_loss = total_val_dice_loss / len(valid_loader)\n",
    "#     avg_val_iou = total_val_iou / len(valid_loader)\n",
    "#     avg_val_accuracy = total_val_accuracy / len(valid_loader)\n",
    "#     avg_val_dice_coeff = 1 - avg_val_dice_loss\n",
    "\n",
    "#     generate_pred_panoptic(cfg, val_outputs)\n",
    "#     pq_res = pq_compute(\n",
    "#         gt_json_file=os.path.join(cfg.DATASET_PATH, cfg.VALID_JSON),\n",
    "#         pred_json_file=os.path.join(cfg.DATASET_PATH, cfg.PRED_JSON),\n",
    "#         gt_folder=os.path.join(cfg.DATASET_PATH, \"output/cityscapes_panoptic_val/\"),\n",
    "#         pred_folder=os.path.join(cfg.DATASET_PATH, cfg.PRED_DIR)\n",
    "#     )\n",
    "#     current_pq = 100 * pq_res['All']['pq']\n",
    "\n",
    "#     epoch_duration = time.time() - start_time\n",
    "\n",
    "#     # Print epoch summary\n",
    "#     print(f\"Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, PQ: {current_pq:.2f}, \"\n",
    "#           f\"Train Dice Coeff: {avg_train_dice_coeff:.4f}, Train IoU: {avg_train_iou:.4f}, Train Acc: {avg_train_accuracy:.4f}, \"\n",
    "#           f\"Val Dice Coeff: {avg_val_dice_coeff:.4f}, Val IoU: {avg_val_iou:.4f}, Val Acc: {avg_val_accuracy:.4f}, \"\n",
    "#           f\"Time Taken: {timedelta(seconds=epoch_duration)}\")\n",
    "\n",
    "#     # Log results to CSV\n",
    "#     with open(metrics_csv_path, \"a\") as f:\n",
    "#         f.write(f\"{epoch+1},{avg_train_loss:.4f},{avg_val_loss:.4f},{current_pq:.2f},\"\n",
    "#                 f\"{100*pq_res['All']['sq']:.2f},{100*pq_res['All']['rq']:.2f},\"\n",
    "#                 f\"{avg_train_dice_coeff:.4f},{avg_train_iou:.4f},{avg_train_accuracy:.4f},{avg_train_dice_loss:.4f},\"\n",
    "#                 f\"{avg_val_dice_coeff:.4f},{avg_val_iou:.4f},{avg_val_accuracy:.4f},{avg_val_dice_loss:.4f}\\n\")\n",
    "\n",
    "#     print(f\"Metrics logged to {metrics_csv_path}\")\n",
    "\n",
    "#     torch.save(efficientps.state_dict(), f\"efficientps_epoch{epoch+1}.pth\")\n",
    "\n",
    "#     if avg_val_loss < best_val_loss:\n",
    "#         best_val_loss = avg_val_loss\n",
    "#         epochs_without_improvement = 0\n",
    "#         print(\"âœ… Best model saved!\")\n",
    "#     else:\n",
    "#         epochs_without_improvement += 1\n",
    "#         print(f\"ðŸš¨ No improvement for {epochs_without_improvement} epochs.\")\n",
    "\n",
    "#     if epochs_without_improvement >= patience:\n",
    "#         print(f\"â¹ï¸ Early stopping triggered after {epoch+1} epochs! Best Validation Loss: {best_val_loss:.4f}\")\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from detectron2.structures import Instances\n",
    "\n",
    "# Load Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "efficientps1 = EffificientPS(cfg)  # Adjust if needed\n",
    "\n",
    "print(\"Before Loading Weights\")\n",
    "# print(torch.mean(efficientps1.instance_head.roi_heads.mask_head.last_conv.weight).item())\n",
    "\n",
    "checkpoint_path = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/efficientps_epoch20.pth\"  # Change this to your model path# Replace with your actual saved epoch file\n",
    "efficientps1.load_state_dict(torch.load(checkpoint_path, map_location=\"cuda\"))\n",
    "efficientps1.to(\"cuda\")\n",
    "efficientps1.eval()\n",
    "# total_params = sum(p.numel() for p in efficientps1.parameters() if p.requires_grad)\n",
    "# print(f\"Total Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "# for name, param in efficientps1.nai.__dict__med_parameters():\n",
    "#     print(name, torch.mean(param).item())  # Print mean weight values\n",
    "# print(\"\\nAfter Loading Weights\")\n",
    "# print(torch.mean(efficientps1.instance_head.roi_heads.mask_head.last_conv.weight).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from detectron2.structures import Instances\n",
    "\n",
    "# Load Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "efficientps1 = EffificientPS(cfg)  # Adjust if needed\n",
    "\n",
    "print(\"Before Loading Weights\")\n",
    "# print(torch.mean(efficientps1.instance_head.roi_heads.mask_head.last_conv.weight).item())\n",
    "\n",
    "checkpoint_path = \"/mnt/e3dbc9b9-6856-470d-84b1-ff55921cd906/sa/EPS_MEDICAL/efficientps_epoch20.pth\"  # Change this to your model path# Replace with your actual saved epoch file\n",
    "efficientps1.load_state_dict(torch.load(checkpoint_path, map_location=\"cuda\"))\n",
    "efficientps1.to(\"cuda\")\n",
    "efficientps1.eval()\n",
    "# total_params = sum(p.numel() for p in efficientps1.parameters() if p.requires_grad)\n",
    "# print(f\"Total Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "# for name, param in efficientps1.nai.__dict__med_parameters():\n",
    "#     print(name, torch.mean(param).item())  # Print mean weight values\n",
    "# print(\"\\nAfter Loading Weights\")\n",
    "# print(torch.mean(efficientps1.instance_head.roi_heads.mask_head.last_conv.weight).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances.get_fields()['pred_masks'].shape,instances.get_fields()['pred_masks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Extract predicted boxes and masks\n",
    "instances = outputs['instance'][0]\n",
    "pred_boxes = instances.get_fields()['pred_boxes'].tensor.cpu().numpy()  # Shape: (N, 4)\n",
    "pred_masks = instances.get_fields()['pred_masks']  # Shape: (N, 1, 28, 28)\n",
    "\n",
    "# Initialize a blank 256x256 mask\n",
    "full_mask = np.zeros((256, 256), dtype=np.uint8)\n",
    "\n",
    "# Create the figure and axis for plotting\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.set_title(\"Final Mask with Bounding Boxes (256Ã—256)\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Plot mask and draw bounding boxes\n",
    "for i in range(pred_masks.shape[0]):  \n",
    "    x1, y1, x2, y2 = map(int, pred_boxes[i])\n",
    "\n",
    "    # Resize mask to bounding box size\n",
    "    resized_mask = F.interpolate(pred_masks[i].unsqueeze(0), size=(y2 - y1, x2 - x1), mode=\"bilinear\", align_corners=False)\n",
    "    resized_mask = resized_mask.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    binary_mask = (resized_mask > 0.5).astype(np.uint8)\n",
    "    full_mask[y1:y2, x1:x2] = np.maximum(full_mask[y1:y2, x1:x2], binary_mask * 255)\n",
    "\n",
    "    # Draw bounding box\n",
    "    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "# Show final mask with boxes\n",
    "ax.imshow(full_mask, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import albumentations as A\n",
    "\n",
    "def plot_segmentation_results(\n",
    "    model, \n",
    "    image_folder, \n",
    "    mask_folder, \n",
    "    cfg, \n",
    "    num_samples=5,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots input images, ground truth masks, predicted semantic masks, \n",
    "    and instance masks + bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model for inference.\n",
    "        image_folder: Path to images directory.\n",
    "        mask_folder: Path to ground-truth masks directory.\n",
    "        cfg: Configuration containing normalization values.\n",
    "        num_samples: Number of random samples to plot.\n",
    "        device: 'cuda' or 'cpu'.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Get list of images\n",
    "    all_images = sorted([f for f in os.listdir(image_folder) if f.endswith(\".png\")])\n",
    "\n",
    "    # Randomly select samples\n",
    "    selected_images = random.sample(all_images, num_samples)\n",
    "\n",
    "    for idx, image_name in enumerate(selected_images):\n",
    "        image_path = os.path.join(image_folder, image_name)\n",
    "\n",
    "        # Get corresponding mask file\n",
    "        mask_name = image_name.replace(\"images\", \"masks\")\n",
    "        mask_path = os.path.join(mask_folder, mask_name)\n",
    "\n",
    "        # Check if corresponding mask exists\n",
    "        if not os.path.exists(mask_path):\n",
    "            print(f\"Warning: Mask not found for {image_name}\")\n",
    "            continue\n",
    "\n",
    "        # Load input image\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)  # (H, W)\n",
    "        image = np.expand_dims(image, axis=-1)                # (H, W, 1)\n",
    "\n",
    "        # Load GT mask\n",
    "        ground_truth_mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)  # (H, W)\n",
    "\n",
    "        # Preprocessing\n",
    "        transform = A.Compose([\n",
    "            A.Resize(height=256, width=256),\n",
    "            A.Normalize(mean=cfg.TRANSFORM.NORMALIZE.MEAN, std=cfg.TRANSFORM.NORMALIZE.STD),\n",
    "        ])\n",
    "        transformed = transform(image=image)\n",
    "        image_tensor = torch.tensor(transformed[\"image\"]).permute(2, 0, 1).float().unsqueeze(0).to(device)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"image\": image_tensor}\n",
    "            outputs, _ = model.shared_step(inputs)\n",
    "\n",
    "        # Extract predicted semantic mask\n",
    "        semantic_logits = outputs['semantic'][0].squeeze(0)\n",
    "        semantic_mask = torch.argmax(semantic_logits, dim=0).cpu().numpy()\n",
    "\n",
    "        # Extract instances\n",
    "        instances =outputs['instance']\n",
    "        if(instances == None):\n",
    "            continue\n",
    "        instances = outputs['instance'][0]\n",
    "        pred_boxes = instances.get_fields()['pred_boxes'].tensor.cpu().numpy()\n",
    "        pred_masks = instances.get_fields()['pred_masks']\n",
    "\n",
    "        # Create full-size instance mask\n",
    "        instance_mask_full = np.zeros((256, 256), dtype=np.uint8)\n",
    "        for i in range(pred_masks.shape[0]):\n",
    "            x1, y1, x2, y2 = map(int, pred_boxes[i])\n",
    "            resized_mask = F.interpolate(pred_masks[i].unsqueeze(0), size=(y2 - y1, x2 - x1), mode=\"bilinear\", align_corners=False)\n",
    "            resized_mask = resized_mask.squeeze().detach().cpu().numpy()\n",
    "            binary_mask = (resized_mask > 0.5).astype(np.uint8)\n",
    "            instance_mask_full[y1:y2, x1:x2] = np.maximum(instance_mask_full[y1:y2, x1:x2], binary_mask * 255)\n",
    "\n",
    "        # ================= Plotting =================\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "\n",
    "        # Original input\n",
    "        axes[0].imshow(image.squeeze(), cmap='gray')\n",
    "        axes[0].set_title(f\"Sample {idx+1}: Input Image\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Ground-truth mask\n",
    "        axes[1].imshow(ground_truth_mask, cmap='gray')\n",
    "        axes[1].set_title(\"Ground Truth Mask\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        # Predicted semantic mask\n",
    "        axes[2].imshow(semantic_mask, cmap='jet')\n",
    "        axes[2].set_title(\"Predicted Semantic Mask\")\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        # Instance mask + bounding boxes\n",
    "        axes[3].imshow(instance_mask_full, cmap='gray')\n",
    "        for i in range(pred_boxes.shape[0]):\n",
    "            x1, y1, x2, y2 = map(int, pred_boxes[i])\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='red', facecolor='none')\n",
    "            axes[3].add_patch(rect)\n",
    "        axes[3].set_title(\"Instance Masks + Boxes\")\n",
    "        axes[3].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_segmentation_results(\n",
    "    model=efficientps1,\n",
    "    image_folder=\"EPS_MEDICAL/Liver_dataset/leftImg8bit/train\",\n",
    "    mask_folder=\"EPS_MEDICAL/Liver_dataset/gtFine/train\",\n",
    "    cfg=cfg,\n",
    "    num_samples=7  # Number of random samples to plot\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
